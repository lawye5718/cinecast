#!/usr/bin/env python3
"""
CineCast å¤§æ¨¡å‹å‰§æœ¬é¢„å¤„ç†å™¨
é˜¶æ®µä¸€ï¼šå‰§æœ¬åŒ–ä¸å¾®åˆ‡ç‰‡ (Script & Micro-chunking)
å®ç°å®è§‚å‰§æœ¬è§£æ -> è‡ªåŠ¨å±•å¼€ä¸ºå¾®åˆ‡ç‰‡å‰§æœ¬
"""

import json
import re
import logging
import requests
import os
import tempfile
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)


def atomic_json_write(path: str, data, **kwargs) -> None:
    """Atomic JSON write: write to a temporary file first, then replace.

    This prevents JSON corruption if the process crashes mid-write.
    """
    dir_name = os.path.dirname(path) or "."
    kwargs.setdefault("ensure_ascii", False)
    kwargs.setdefault("indent", 2)
    fd, tmp_path = tempfile.mkstemp(suffix=".tmp", dir=dir_name)
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, **kwargs)
        os.replace(tmp_path, path)
    except BaseException:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass
        raise


def repair_json_array(raw: str) -> Optional[List[Dict]]:
    """Attempt to repair a truncated or malformed JSON array.

    Tries progressively more aggressive strategies:
    1. Strip trailing garbage after the last ``}`` and close the array.
    2. Use regex to salvage individual JSON objects.

    Returns ``None`` if nothing can be recovered.
    """
    # Strategy 1: find last complete object and close the array
    raw = raw.strip()
    if raw.startswith("["):
        last_brace = raw.rfind("}")
        if last_brace > 0:
            candidate = raw[: last_brace + 1].rstrip().rstrip(",") + "\n]"
            try:
                result = json.loads(candidate)
                if isinstance(result, list) and result:
                    return result
            except json.JSONDecodeError:
                pass

    # Strategy 2: regex salvage individual entries
    return salvage_json_entries(raw)


def salvage_json_entries(raw: str) -> Optional[List[Dict]]:
    """Use regex to extract valid script entries from broken JSON text.

    Each entry is expected to have at least ``speaker`` and ``content`` fields.
    """
    pattern = re.compile(
        r'\{\s*'
        r'"(?:type)"\s*:\s*"([^"]*)"\s*,\s*'
        r'"(?:speaker)"\s*:\s*"([^"]*)"\s*,\s*'
        r'"(?:gender)"\s*:\s*"([^"]*)"\s*,\s*'
        r'"(?:emotion|instruct)"\s*:\s*"([^"]*)"\s*,\s*'
        r'"(?:content)"\s*:\s*"([^"]*)"',
        re.DOTALL,
    )
    entries = []
    for m in pattern.finditer(raw):
        entries.append({
            "type": m.group(1) or "narration",
            "speaker": m.group(2) or "narrator",
            "gender": m.group(3) or "unknown",
            "emotion": m.group(4) or "å¹³é™",
            "content": m.group(5) or "",
        })

    if not entries:
        # Looser pattern: just find speaker + content
        loose = re.compile(
            r'"speaker"\s*:\s*"([^"]*)"\s*[,}].*?"content"\s*:\s*"([^"]*)"',
            re.DOTALL,
        )
        for m in loose.finditer(raw):
            entries.append({
                "type": "narration",
                "speaker": m.group(1) or "narrator",
                "gender": "unknown",
                "emotion": "å¹³é™",
                "content": m.group(2) or "",
            })

    return entries if entries else None


def merge_consecutive_narrators(script: List[Dict], max_chars: int = 800) -> List[Dict]:
    """Merge consecutive narrator entries that share the same emotion.

    This reduces TTS startup overhead and avoids fragmented short sentences
    that cause jarring tonal shifts.
    """
    if not script:
        return script

    merged: List[Dict] = []
    for entry in script:
        if (
            merged
            and entry.get("speaker") == "narrator"
            and merged[-1].get("speaker") == "narrator"
            and entry.get("emotion", "å¹³é™") == merged[-1].get("emotion", "å¹³é™")
            and entry.get("type") == merged[-1].get("type")
            and len(merged[-1].get("content", "")) + len(entry.get("content", "")) <= max_chars
        ):
            merged[-1]["content"] = merged[-1]["content"] + entry["content"]
            # Keep the longer pause
            merged[-1]["pause_ms"] = max(
                merged[-1].get("pause_ms", 0), entry.get("pause_ms", 0)
            )
        else:
            merged.append(entry.copy())

    return merged

class LLMScriptDirector:
    def __init__(self, ollama_url="http://127.0.0.1:11434", use_local_mlx_lm=False):
        self.api_url = f"{ollama_url}/api/chat"
        self.model_name = "qwen14b-pro"
        self.max_chars_per_chunk = 60 # å¾®åˆ‡ç‰‡çº¢çº¿
        self.use_local_mlx_lm = use_local_mlx_lm
        
        # Context sliding window state
        self._prev_characters: List[str] = []
        self._prev_tail_entries: List[Dict] = []
        
        # æµ‹è¯•Ollamaè¿æ¥
        self._test_ollama_connection()
    
    def _test_ollama_connection(self):
        """æµ‹è¯•OllamaæœåŠ¡è¿æ¥"""
        try:
            response = requests.get(f"{self.api_url.replace('/api/chat', '')}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("âœ… OllamaæœåŠ¡è¿æ¥æ­£å¸¸")
                return True
            else:
                logger.warning("âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸")
                return False
        except Exception as e:
            logger.warning(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
            return False
    
    def _try_ollama_qwen(self) -> bool:
        """å°è¯•ä½¿ç”¨Ollamaçš„Qwen14Bæ¨¡å‹"""
        try:
            import subprocess
            result = subprocess.run(
                ["ollama", "list"], 
                capture_output=True, 
                text=True, 
                timeout=10
            )
            
            if result.returncode == 0 and "qwen14b-pro" in result.stdout:
                logger.info("âœ… æˆåŠŸæ£€æµ‹åˆ°æœ¬åœ°Ollama Qwen14Bæ¨¡å‹")
                self.model_type = "ollama"
                self.model_name = "qwen14b-pro"
                return True
            else:
                logger.info("æœªæ‰¾åˆ°Ollama Qwen14Bæ¨¡å‹")
                return False
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥Ollamaæ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
    
    def _chunk_text_for_llm(self, text: str, max_length: int = 1500) -> List[str]:
        """ğŸŒŸ é˜²æ­¢ç« èŠ‚è¿‡é•¿ï¼ŒæŒ‰æ®µè½åˆ‡åˆ†ä¸ºå®‰å…¨å¤§å°ç»™ LLM å¤„ç†"""
        paragraphs = text.split('\n')
        chunks, current_chunk = [], ""
        for para in paragraphs:
            if not para.strip(): continue
            if len(current_chunk) + len(para) > max_length and current_chunk:
                chunks.append(current_chunk)
                current_chunk = para + "\n"
            else:
                current_chunk += para + "\n"
        if current_chunk:
            chunks.append(current_chunk)
        return chunks
    
    def parse_and_micro_chunk(self, text: str, chapter_prefix: str = "chunk") -> List[Dict]:
        """å®è§‚å‰§æœ¬è§£æ -> è‡ªåŠ¨å±•å¼€ä¸ºå¾®åˆ‡ç‰‡å‰§æœ¬
        
        Args:
            text: å¾…å¤„ç†çš„ç« èŠ‚æ–‡æœ¬
            chapter_prefix: ç« èŠ‚åç§°å‰ç¼€ï¼Œç”¨äºé¿å…æ–‡ä»¶åå†²çª
        """
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå®è§‚å‰§æœ¬
        macro_script = self.parse_text_to_script(text)
        micro_script = []
        chunk_id = 1
        
        for unit in macro_script:
            content = unit.get("content", "")
            if not content or not content.strip():
                continue

            # å®æ–½å¾®åˆ‡ç‰‡
            raw_sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š])', content)
            chunks, temp = [], ""
            for part in raw_sentences:
                if not part.strip(): continue
                if re.match(r'^[ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š]$', part.strip()):
                    chunks.append(temp + part)
                    temp = ""
                else:
                    temp += part
                    if len(temp) >= self.max_chars_per_chunk:
                        chunks.append(temp)
                        temp = ""
            if temp: chunks.append(temp)
            
            # æ¸…ç†ç©ºå—å¹¶è®¡ç®—åœé¡¿
            valid_chunks = [c.strip() for c in chunks if c.strip()]

            # ğŸŒŸ å…œåº•é€»è¾‘ï¼šå¦‚æœæ­£åˆ™åˆ‡åˆ†åæ— æœ‰æ•ˆå—ï¼ŒæŒ‰æ¯60å­—ç¡¬åˆ‡
            if not valid_chunks and content.strip():
                hard_cut_size = self.max_chars_per_chunk
                stripped = content.strip()
                valid_chunks = [
                    stripped[i:i + hard_cut_size]
                    for i in range(0, len(stripped), hard_cut_size)
                ]
                logger.warning(
                    f"âš ï¸ æ­£åˆ™åˆ‡åˆ†æ— ç»“æœï¼Œå·²æŒ‰æ¯{hard_cut_size}å­—ç¡¬åˆ‡: "
                    f"'{content[:30]}...'"
                )

            for idx, chunk in enumerate(valid_chunks):
                is_para_end = (idx == len(valid_chunks) - 1)
                pause_ms = self._calculate_pause(chunk, is_para_end)
                
                # ğŸŒŸ ä¿®å¤ï¼šå°†ç« èŠ‚åç§°å‰ç¼€åŠ å…¥IDï¼Œæœç»æ–‡ä»¶è¦†ç›–ï¼
                micro_script.append({
                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                    "type": unit["type"],
                    "speaker": unit["speaker"],
                    "gender": unit.get("gender", "male"),
                    "content": chunk,
                    "pause_ms": pause_ms
                })
                chunk_id += 1
                
        return micro_script

    def _calculate_pause(self, chunk_text: str, is_para_end: bool) -> int:
        """æå‰è®¡ç®—å¥½ç‰©ç†åœé¡¿æ—¶é—´"""
        if is_para_end: return 1000
        if chunk_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')): return 600
        elif chunk_text.endswith(('ï¼›', ';')): return 400
        elif chunk_text.endswith(('ï¼Œ', 'ã€', ',', 'ï¼š', ':')): return 250
        else: return 100

    def parse_text_to_script(self, text: str) -> List[Dict]:
        """é˜¶æ®µä¸€ï¼šå®è§‚å‰§æœ¬è§£æï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        
        Implements a context sliding window: each chunk receives the previous
        chunk's cast list and last three entries as context so that character
        names and speaking styles stay consistent across slices.
        """
        # ğŸŒŸ ä¿®å¤æˆªæ–­æ¼æ´ï¼šæŒ‰æ®µè½åˆ‡åˆ†é•¿ç« èŠ‚
        text_chunks = self._chunk_text_for_llm(text)
        full_script = []
        
        for i, chunk in enumerate(text_chunks):
            logger.info(f"   ğŸ§  æ­£åœ¨è§£æå‰§æƒ…ç‰‡æ®µ {i+1}/{len(text_chunks)}...")
            
            # Build context from previous chunk
            context_parts: List[str] = []
            if self._prev_characters:
                context_parts.append(
                    "å‰ä¸€æ®µå‡ºåœºè§’è‰²: " + ", ".join(self._prev_characters)
                )
            if self._prev_tail_entries:
                try:
                    tail_json = json.dumps(
                        self._prev_tail_entries, ensure_ascii=False
                    )
                    context_parts.append(
                        "\nPrevious section ended with:\n" + tail_json
                    )
                except Exception:
                    pass
            
            chunk_script = self._request_ollama(chunk, context="\n".join(context_parts) if context_parts else None)
            
            # Update sliding window state
            if chunk_script:
                speakers = {
                    e.get("speaker")
                    for e in chunk_script
                    if e.get("speaker") and e.get("speaker") != "narrator"
                }
                if speakers:
                    self._prev_characters = list(speakers)
                self._prev_tail_entries = chunk_script[-3:]
            
            full_script.extend(chunk_script)
        
        # Merge consecutive narrators to reduce TTS overhead
        full_script = merge_consecutive_narrators(full_script)
        
        # å¦‚æœè§£æç»“æœä¸ºç©ºï¼Œç›´æ¥æŠ¥é”™é€€å‡º
        if not full_script or len(full_script) == 0:
            raise RuntimeError("âŒ å‰§æœ¬è§£æç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡æœ¬å’Œå¤§æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸ã€‚")
            
        return full_script
    
    def generate_chapter_recap(self, prev_chapter_text: str) -> str:
        """
        ä¸“é—¨ç”¨äºç”Ÿæˆå‰æƒ…æ‘˜è¦å’Œæ‚¬å¿µé’©å­
        """
        system_prompt = """
        ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æœ‰å£°ä¹¦å‰§æœ¬ç¼–è¾‘ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸€ç« å†…å®¹ï¼Œå†™ä¸€æ®µä¸è¶…è¿‡100å­—çš„"å‰æƒ…æ‘˜è¦"ã€‚
        è¦æ±‚ï¼š
        1. æç‚¼æœ€æ ¸å¿ƒçš„å‰§æƒ…å†²çªæˆ–ç²¾åã€‚
        2. è¯­è¨€é£æ ¼è¦å…·æœ‰æ‚¬ç–‘æ„Ÿå’Œç”µå½±æ„Ÿï¼ˆç±»ä¼¼äºç¾å‰§å¼€å¤´çš„ "Previously on..."ï¼‰ã€‚
        3. æœ€åä¸€å¥å¿…é¡»æ˜¯ä¸€ä¸ªå¼•å‡ºä¸‹ä¸€ç« çš„"æ‚¬å¿µé’©å­"ï¼ˆä¾‹å¦‚ï¼š"ç„¶è€Œï¼Œå¥¹å¹¶æ²¡æœ‰æ„è¯†åˆ°ï¼ŒçœŸæ­£çš„å±é™©æ‰åˆšåˆšé™ä¸´â€¦â€¦"ï¼‰ã€‚
        4. åªè¾“å‡ºæ‘˜è¦æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•æ ¼å¼å’Œå‰ç¼€ã€‚
        """
        
        # ä¸ºäº†é˜²æ­¢è¾“å…¥è¿‡é•¿ï¼Œæˆªå–ä¸Šä¸€ç« çš„ååŠéƒ¨åˆ†æˆ–é™åˆ¶æ€»å­—æ•°
        input_text = prev_chapter_text[-2000:] if len(prev_chapter_text) > 2000 else prev_chapter_text
        
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ä¸Šä¸€ç« å†…å®¹ï¼š\n{input_text}"}
            ],
            "stream": False,
            "options": {"temperature": 0.5}
        }
        
        try:
            response = requests.post(self.api_url, json=payload, timeout=60)
            response.raise_for_status()
            return response.json().get('message', {}).get('content', '').strip()
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _request_ollama(self, text_chunk: str, *, context: Optional[str] = None) -> List[Dict]:
        """å‘Ollamaå‘é€å•ä¸ªæ–‡æœ¬å—è¯·æ±‚

        Args:
            text_chunk: The raw text to convert into a script.
            context: Optional sliding-window context from the previous chunk
                     (character list + tail entries) to maintain consistency.
        """
        system_prompt = """
        ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æœ‰å£°ä¹¦å¯¼æ¼”å…¼æ•°æ®æ¸…æ´—ä¸“å®¶ï¼Œè´Ÿè´£å°†åŸå§‹å°è¯´æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„å½•éŸ³å‰§æœ¬ã€‚
        ä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹å››å¤§çºªå¾‹ï¼Œä»»ä½•è¿åéƒ½å°†å¯¼è‡´ç³»ç»Ÿå´©æºƒï¼š

        ã€ä¸€ã€ ç»å¯¹å¿ å®åŸåˆ™ï¼ˆIron Ruleï¼‰ã€‘
        - å¿…é¡» 100% é€å­—ä¿ç•™åŸæ–‡å†…å®¹ï¼
        - ä¸¥ç¦ä»»ä½•å½¢å¼çš„æ¦‚æ‹¬ã€æ”¹å†™ã€ç¼©å†™ã€ç»­å†™æˆ–æ¶¦è‰²ï¼
        - ä¸¥ç¦è‡ªè¡Œæ·»åŠ åŸæ–‡ä¸­ä¸å­˜åœ¨çš„å°è¯æˆ–åŠ¨ä½œæå†™ï¼
        - ä¸¥ç¦åœ¨ content ä¸­ä¿ç•™å½’å±æ ‡ç­¾ï¼ˆå¦‚"ä»–è¯´"ã€"å¥¹å«é“"ï¼‰ï¼Œå½’å±ä¿¡æ¯åªèƒ½å‡ºç°åœ¨ speaker å­—æ®µï¼

        ã€äºŒã€ å­—ç¬¦å‡€åŒ–åŸåˆ™ã€‘
        - å‰”é™¤æ‰€æœ‰ä¸å¯å‘éŸ³çš„ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚ Emojiè¡¨æƒ…ã€Markdownæ ‡è®° * _ ~ #ã€åˆ¶è¡¨ç¬¦ \tã€ä¸å¯è§æ§åˆ¶å­—ç¬¦ï¼‰ã€‚
        - ä»…ä¿ç•™åŸºç¡€æ ‡ç‚¹ç¬¦å·ï¼ˆï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€""''ï¼ˆï¼‰ï¼‰ã€‚
        - æ•°å­—ã€è‹±æ–‡å­—æ¯å…è®¸ä¿ç•™ï¼Œä½†ç¦æ­¢å‡ºç°å¤æ‚çš„æ•°å­¦å…¬å¼ç¬¦å·ã€‚

        ã€ä¸‰ã€ ç²’åº¦æ‹†åˆ†åŸåˆ™ã€‘
        - å¿…é¡»å°†"å¯¹ç™½"å’Œ"æ—ç™½/åŠ¨ä½œæå†™"ä¸¥æ ¼å‰¥ç¦»ä¸ºç‹¬ç«‹çš„å¯¹è±¡ï¼
        - ä¾‹å¦‚åŸæ–‡ï¼š"ä½ å¥½ï¼Œ"è€æ¸”å¤«ç¬‘ç€è¯´ã€‚
          å¿…é¡»æ‹†åˆ†ä¸ºä¸¤ä¸ªå¯¹è±¡ï¼š1. è§’è‰²å¯¹ç™½("ä½ å¥½ï¼Œ") 2. æ—ç™½æè¿°("è€æ¸”å¤«ç¬‘ç€è¯´ã€‚")

        ã€å››ã€ JSON æ ¼å¼è§„èŒƒã€‘
        å¿…é¡»ä¸”åªèƒ½è¾“å‡ºåˆæ³•çš„ JSON æ•°ç»„ï¼Œç¦æ­¢ä»»ä½•è§£é‡Šæ€§å‰è¨€æˆ–åç¼€ï¼ˆå¦‚"å¥½çš„ï¼Œä»¥ä¸‹æ˜¯..."ï¼‰ï¼Œç¦æ­¢è¾“å‡º Markdown ä»£ç å—æ ‡è®°ï¼ˆ```jsonï¼‰ã€‚
        æ•°ç»„å…ƒç´ å­—æ®µè¦æ±‚ï¼š
        - "type": ä»…é™ "title"(ç« èŠ‚å), "subtitle"(å°æ ‡é¢˜), "narration"(æ—ç™½), "dialogue"(å¯¹ç™½)ã€‚
        - "speaker": å¯¹ç™½å¡«å…·ä½“çš„è§’è‰²åï¼ˆéœ€æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­å¹¶ä¿æŒå…¨ä¹¦ç»Ÿä¸€ï¼‰ï¼›æ—ç™½å’Œæ ‡é¢˜ç»Ÿä¸€å¡« "narrator"ã€‚
        - "gender": ä»…é™ "male"ã€"female" æˆ– "unknown"ã€‚å¯¹ç™½è¯·æ¨æµ‹æ€§åˆ«ï¼›æ—ç™½å›ºå®šä¸º "male"ã€‚
        - "emotion": æƒ…æ„Ÿæ ‡ç­¾ï¼ˆå¦‚"å¹³é™"ã€"æ¿€åŠ¨"ã€"æ²§æ¡‘/å¹æ¯"ã€"æ„¤æ€’"ã€"æ‚²ä¼¤"ç­‰ï¼‰ï¼Œç”¨äºæœªæ¥è¯­éŸ³åˆæˆçš„æƒ…æ„Ÿæ§åˆ¶ã€‚
        - "content": çº¯å‡€çš„æ–‡æœ¬å†…å®¹ã€‚å¦‚æœ type æ˜¯ "dialogue"ï¼Œå¿…é¡»å»æ‰æœ€å¤–å±‚çš„å¼•å·ï¼ˆå¦‚""æˆ–""ï¼‰ã€‚

        ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼ˆOne-Shotï¼‰ã€‘
        [
          {
            "type": "narration",
            "speaker": "narrator",
            "gender": "male",
            "emotion": "å¹³é™",
            "content": "å¤œå¹•é™ä¸´ï¼Œæ¸¯å£çš„ç¯ç«å¼€å§‹é—ªçƒã€‚"
          },
          {
            "type": "dialogue",
            "speaker": "è€æ¸”å¤«",
            "gender": "male",
            "emotion": "æ²§æ¡‘/å¹æ¯",
            "content": "ä½ ç›¸ä¿¡å‘½è¿å—ï¼Ÿ"
          },
          {
            "type": "narration",
            "speaker": "narrator",
            "gender": "male",
            "emotion": "å¹³é™",
            "content": "è€æ¸”å¤«è¯´é“ã€‚"
          }
        ]
        """

        user_content = "è¯·ä¸¥æ ¼æŒ‰ç…§è§„èŒƒï¼Œå°†ä»¥ä¸‹æ–‡æœ¬æ‹†è§£ä¸ºçº¯å‡€çš„ JSON å‰§æœ¬ï¼ˆç»ä¸æ”¹å†™åŸæ„ï¼‰ï¼š\n\n"
        if context:
            user_content += f"ã€ä¸Šæ–‡å‚è€ƒï¼ˆä»…ä¾›è§’è‰²ä¸€è‡´æ€§å‚è€ƒï¼Œä¸è¦ç¿»è¯‘æ­¤æ®µï¼‰ã€‘\n{context}\n\n"
        user_content += text_chunk

        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "format": "json",
            "stream": False,
            "keep_alive": "10m",
            "options": {
                "num_ctx": 8192,
                "temperature": 0.0,
                "top_p": 0.1
            }
        }

        try:
            response = requests.post(self.api_url, json=payload, timeout=180)
            response.raise_for_status()
            content = response.json().get('message', {}).get('content', '[]')

            # ğŸŒŸ é¢„å¤„ç†ï¼šæ¸…æ´—å®é™…æ§åˆ¶å­—ç¬¦ï¼ˆé˜²æ­¢ LLM è¾“å‡ºç ´å JSON è§£æï¼‰
            # Only strip real control characters; keep escaped sequences
            # like \n and \t inside JSON strings intact.
            content = content.replace('\t', ' ').replace('\r', '')

            # Strip Markdown code-block wrappers the LLM may hallucinate
            content = re.sub(r'^```(?:json)?\s*', '', content.strip(), flags=re.IGNORECASE)
            content = re.sub(r'\s*```$', '', content.strip())

            try:
                script = json.loads(content)
            except json.JSONDecodeError:
                logger.warning("âš ï¸ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤æˆªæ–­çš„ JSON ...")
                script = repair_json_array(content)
                if script is None:
                    raise RuntimeError(
                        f"âŒ å¤§æ¨¡å‹è¿”å›çš„ JSON æ— æ³•è§£æä¸”ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºã€‚åŸå§‹å†…å®¹: {content[:200]}"
                    )
                return self._validate_script_elements(script)

            if isinstance(script, list):
                return self._validate_script_elements(script)
            if isinstance(script, dict):
                for value in script.values():
                    if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
                        return self._validate_script_elements(value)
            raise RuntimeError(
                f"âŒ å¤§æ¨¡å‹è¿”å›äº†éé¢„æœŸçš„ JSON ç»“æ„ï¼ˆæ—¢éæ•°ç»„ä¹ŸéåŒ…å«æ•°ç»„çš„å­—å…¸ï¼‰ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è¾“å‡ºã€‚åŸå§‹å†…å®¹: {content[:200]}"
            )

        except RuntimeError:
            raise
        except Exception as e:
            raise RuntimeError(f"âŒ Ollama è§£æå¤±è´¥: {e}") from e
    
    def _validate_script_elements(self, script: List[Dict]) -> List[Dict]:
        """éªŒè¯å¹¶ä¿®å¤è„šæœ¬å…ƒç´ ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ"""
        required_fields = ['type', 'speaker', 'content']
        validated_script = []
        
        for i, element in enumerate(script):
            # ç¡®ä¿æ˜¯å­—å…¸ç±»å‹
            if not isinstance(element, dict):
                logger.warning(f"âš ï¸ è„šæœ¬å…ƒç´  {i} ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡: {element}")
                continue
                
            # æ£€æŸ¥å¹¶è¡¥å……ç¼ºå¤±çš„å­—æ®µ
            fixed_element = element.copy()
            
            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            for field in required_fields:
                if field not in fixed_element:
                    if field == 'type':
                        fixed_element['type'] = 'narration'  # é»˜è®¤ä¸ºæ—ç™½
                    elif field == 'speaker':
                        fixed_element['speaker'] = 'narrator'  # é»˜è®¤è¯´è¯è€…
                    elif field == 'content':
                        fixed_element['content'] = ''  # ç©ºå†…å®¹
                    logger.warning(f"âš ï¸ è¡¥å……ç¼ºå¤±å­—æ®µ '{field}' åœ¨å…ƒç´  {i}: {element}")
            
            # å¼ºåŒ–ä¿®å¤é€»è¾‘ï¼šå¤„ç† None å€¼
            if fixed_element.get('speaker') is None:
                fixed_element['speaker'] = 'narrator'
                logger.warning(f"âš ï¸ ä¿®å¤ None å€¼å­—æ®µ 'speaker' åœ¨å…ƒç´  {i}")
            if fixed_element.get('gender') is None:
                fixed_element['gender'] = 'unknown'
                logger.warning(f"âš ï¸ ä¿®å¤ None/ç¼ºå¤±å­—æ®µ 'gender' åœ¨å…ƒç´  {i}")
            
            # ç¡®ä¿ gender å­—æ®µå­˜åœ¨ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰
            if 'gender' not in fixed_element:
                fixed_element['gender'] = 'unknown'
                logger.warning(f"âš ï¸ è¡¥å……ç¼ºå¤±å­—æ®µ 'gender' åœ¨å…ƒç´  {i}: {element}")
            
            # ç¡®ä¿ emotion å­—æ®µå­˜åœ¨
            if 'emotion' not in fixed_element:
                fixed_element['emotion'] = 'å¹³é™'
            
            validated_script.append(fixed_element)
            
        return validated_script
    

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    director = LLMScriptDirector()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
ç¬¬ä¸€ç«  å‡¯å¤«æ‹‰ç»´å…‹çš„é£é›ª

å¤œå¹•é™ä¸´ï¼Œæ¸¯å£çš„ç¯ç«å¼€å§‹é—ªçƒã€‚

"ä½ ç›¸ä¿¡å‘½è¿å—ï¼Ÿ"è€æ¸”å¤«è¯´é“ã€‚

å¹´è½»äººæ‘‡æ‘‡å¤´ï¼š"æˆ‘åªç›¸ä¿¡æµ·ã€‚"

è¿œå¤„ä¼ æ¥æ±½ç¬›å£°ï¼Œåˆ’ç ´äº†å¯‚é™çš„å¤œç©ºã€‚
"""
    
    script = director.parse_text_to_script(test_text)
    print("è§£æç»“æœ:")
    for i, unit in enumerate(script, 1):
        print(f"{i}. {unit}")