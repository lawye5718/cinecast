#!/usr/bin/env python3
"""
CineCast å¤§æ¨¡å‹å‰§æœ¬é¢„å¤„ç†å™¨
é˜¶æ®µä¸€ï¼šå‰§æœ¬åŒ–ä¸å¾®åˆ‡ç‰‡ (Script & Micro-chunking)
å®ç°å®è§‚å‰§æœ¬è§£æ -> è‡ªåŠ¨å±•å¼€ä¸ºå¾®åˆ‡ç‰‡å‰§æœ¬
"""

import json
import re
import logging
import os
import tempfile
import time
from typing import List, Dict, Optional
from openai import OpenAI

logger = logging.getLogger(__name__)


def atomic_json_write(path: str, data, **kwargs) -> None:
    """Atomic JSON write: write to a temporary file first, then replace.

    This prevents JSON corruption if the process crashes mid-write.
    """
    dir_name = os.path.dirname(path) or "."
    kwargs.setdefault("ensure_ascii", False)
    kwargs.setdefault("indent", 2)
    fd, tmp_path = tempfile.mkstemp(suffix=".tmp", dir=dir_name)
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, **kwargs)
        os.replace(tmp_path, path)
    except BaseException:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass
        raise


def repair_json_array(raw: str) -> Optional[List[Dict]]:
    """Attempt to repair a truncated or malformed JSON array.

    Tries progressively more aggressive strategies:
    1. Strip trailing garbage after the last ``}`` and close the array.
    2. Use regex to salvage individual JSON objects.

    Returns ``None`` if nothing can be recovered.
    """
    # Strategy 1: find last complete object and close the array
    raw = raw.strip()
    if raw.startswith("["):
        last_brace = raw.rfind("}")
        if last_brace > 0:
            candidate = raw[: last_brace + 1].rstrip().rstrip(",") + "\n]"
            try:
                result = json.loads(candidate)
                if isinstance(result, list) and result:
                    return result
            except json.JSONDecodeError:
                pass

    # Strategy 2: regex salvage individual entries
    return salvage_json_entries(raw)


def _extract_fields_from_object(obj_text: str) -> Optional[Dict]:
    """Extract known fields from a single JSON object text in any order.

    Uses individual per-field regexes so that field ordering does not matter.
    Returns a dict with defaults for missing fields, or ``None`` if neither
    ``speaker`` nor ``content`` could be found.
    """
    field_re = re.compile(r'"(\w+)"\s*:\s*"([^"]*)"')
    fields: Dict[str, str] = {}
    for m in field_re.finditer(obj_text):
        fields[m.group(1)] = m.group(2)

    # Map known aliases
    speaker = fields.get("speaker", "")
    content = fields.get("content", "")
    if not speaker and not content:
        return None

    return {
        "type": fields.get("type", "narration") or "narration",
        "speaker": speaker or "narrator",
        "gender": fields.get("gender", "unknown") or "unknown",
        "emotion": fields.get("emotion") or fields.get("instruct") or "å¹³é™",
        "content": content or "",
    }


def salvage_json_entries(raw: str) -> Optional[List[Dict]]:
    """Use regex to extract valid script entries from broken JSON text.

    Each entry is expected to have at least ``speaker`` and ``content`` fields.
    Uses order-independent field extraction so that reordered or extra-spaced
    LLM output can still be recovered.
    """
    # Find all brace-delimited object candidates
    obj_pattern = re.compile(r'\{[^{}]+\}', re.DOTALL)
    entries = []
    for m in obj_pattern.finditer(raw):
        entry = _extract_fields_from_object(m.group(0))
        if entry and entry.get("content"):
            entries.append(entry)

    if not entries:
        # Looser pattern: just find speaker + content anywhere
        loose = re.compile(
            r'"speaker"\s*:\s*"([^"]*)"\s*[,}].*?"content"\s*:\s*"([^"]*)"',
            re.DOTALL,
        )
        for m in loose.finditer(raw):
            entries.append({
                "type": "narration",
                "speaker": m.group(1) or "narrator",
                "gender": "unknown",
                "emotion": "å¹³é™",
                "content": m.group(2) or "",
            })

    return entries if entries else None


def merge_consecutive_narrators(script: List[Dict], max_chars: int = 800) -> List[Dict]:
    """Merge consecutive narrator entries that share the same emotion.

    This reduces TTS startup overhead and avoids fragmented short sentences
    that cause jarring tonal shifts.
    """
    if not script:
        return script

    merged: List[Dict] = []
    for entry in script:
        if (
            merged
            and entry.get("speaker") == "narrator"
            and merged[-1].get("speaker") == "narrator"
            and entry.get("emotion", "å¹³é™") == merged[-1].get("emotion", "å¹³é™")
            and entry.get("type") == merged[-1].get("type")
            and len(merged[-1].get("content", "")) + len(entry.get("content", "")) <= max_chars
        ):
            merged[-1]["content"] = merged[-1]["content"] + entry["content"]
            # Keep the longer pause
            merged[-1]["pause_ms"] = max(
                merged[-1].get("pause_ms", 0), entry.get("pause_ms", 0)
            )
        else:
            merged.append(entry.copy())

    return merged

class LLMScriptDirector:
    # ğŸŒŸ é«˜é˜¶è§’è‰²éŸ³è‰²æ˜ å°„è¡¨ (Voice Archetype Mapping)
    VOICE_ARCHETYPES = {
        "intellectual": "Clear, articulate, mid-range voice, steady pacing, calm and intellectual.",
        "villain_sly": "Slightly nasal, fast-paced voice, bright tone, with a hint of sarcasm.",
        "melancholic": "Breathier, soft voice, melancholic undertones, slow and emotional.",
        "authoritative": "Resonant, deep baritone, slow and authoritative, gravelly texture.",
        "innocent": "Bright, high-pitched, energetic and innocent, clear enunciation.",
    }

    def __init__(self, api_key=None, model_name=None, base_url=None, global_cast=None, cast_db_path=None, **kwargs):
        if kwargs:
            logger.warning(f"âš ï¸ LLMScriptDirector æ”¶åˆ°æœªè¯†åˆ«çš„å‚æ•°ï¼ˆå·²å¿½ç•¥ï¼‰: {list(kwargs.keys())}")
        self.api_key = api_key or os.environ.get("DASHSCOPE_API_KEY", "")
        self.model_name = model_name or "qwen-flash"
        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        
        # ğŸŒŸ ä¼˜åŒ–ï¼šä½¿ç”¨æ ‡å‡† OpenAI SDK å®¢æˆ·ç«¯ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ LLM é…ç½®
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=120.0,
        )
        
        self.max_chars_per_chunk = 150 # ğŸ¯ ä¿®æ”¹ç‚¹ï¼šå¾®åˆ‡ç‰‡çº¢çº¿è°ƒæ•´ä¸º 150 å­—
        self.pure_narrator_chunk_limit = 100  # çº¯å‡€æ—ç™½æ¨¡å¼åˆ‡ç‰‡ä¸Šé™ï¼ˆæ›´é•¿æ›´æµç•…ï¼‰
        self.global_cast = global_cast or {}  # ğŸŒŸ å¤–è„‘å…¨å±€è§’è‰²è®¾å®šé›†
        
        # Context sliding window state
        self._prev_characters: List[str] = []
        self._prev_tail_entries: List[Dict] = []
        self._local_session_cast: Dict[str, str] = {}  # ğŸŒŸ å±€éƒ¨ä¼šè¯è§’è‰²éŸ³è‰²è¡¨ï¼ˆè·¨ chunk éŸ³è‰²ä¸€è‡´æ€§ï¼‰

        # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§æŒä¹…åŒ– (Voice Consistency Persistence)
        self.cast_db_path = cast_db_path or os.path.join("workspace", "cast_profiles.json")
        self.cast_profiles: Dict[str, Dict] = self._load_cast_profiles()
        
        # æµ‹è¯• Qwen API è¿æ¥
        self._test_api_connection()

    # ------------------------------------------------------------------
    # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§æŒä¹…åŒ– (Voice Consistency Persistence)
    # ------------------------------------------------------------------

    def _load_cast_profiles(self) -> Dict[str, Dict]:
        """åŠ è½½å·²ä¿å­˜çš„è§’è‰²éŸ³è‰²åº“"""
        if os.path.exists(self.cast_db_path):
            try:
                with open(self.cast_db_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"âš ï¸ åŠ è½½è§’è‰²éŸ³è‰²åº“å¤±è´¥: {e}")
        return {}

    def _save_cast_profile(self, name: str, gender: str, description: str) -> None:
        """å‘ç°æ–°è§’è‰²æˆ–æ›´æ–°è§’è‰²æ—¶æŒä¹…åŒ–"""
        if name not in self.cast_profiles:
            self.cast_profiles[name] = {
                "gender": gender,
                "voice_instruction": description,
            }
            os.makedirs(os.path.dirname(self.cast_db_path) or ".", exist_ok=True)
            atomic_json_write(self.cast_db_path, self.cast_profiles)

    def _update_cast_db(self, script_list: List[Dict]) -> None:
        """è§£æå®Œä¸€ä¸ª chunk åï¼Œæå–æ–°è§’è‰²å¹¶æŒä¹…åŒ–"""
        updated = False
        for item in script_list:
            speaker = item.get("speaker")
            if not speaker or speaker == "narrator":
                continue
            emotion = item.get("emotion", "")
            gender = item.get("gender", "unknown")
            # æå–æ‹¬å·å†…çš„è‹±æ–‡æè¿°ï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…æ›´å¯é ï¼‰
            if speaker not in self.cast_profiles:
                m = re.search(r'\(([^)]+)\)', emotion)
                if m:
                    self.cast_profiles[speaker] = {
                        "gender": gender,
                        "voice_instruction": m.group(1),
                    }
                    updated = True

        if updated:
            os.makedirs(os.path.dirname(self.cast_db_path) or ".", exist_ok=True)
            atomic_json_write(self.cast_db_path, self.cast_profiles)

    # ------------------------------------------------------------------
    # ğŸŒŸ é«˜é˜¶è§’è‰²éŸ³è‰²æ˜ å°„è¡¨ Prompt ç”Ÿæˆ
    # ------------------------------------------------------------------

    def _get_archetype_prompt(self) -> str:
        """ç”Ÿæˆæ³¨å…¥ System Prompt çš„éŸ³è‰²æ˜ å°„æŒ‡å—"""
        guidelines = "\n".join(
            [f"  - {k}: {v}" for k, v in self.VOICE_ARCHETYPES.items()]
        )
        return (
            "\nã€éŸ³è‰²è®¾è®¡å‚è€ƒæ‰‹å†Œã€‘\n"
            "å½“ä¸ºæ–°è§’è‰²ç”Ÿæˆ (Acoustic Description) æ—¶ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä»¥ä¸‹æ–‡å­¦åŸå‹æè¿°è¯ï¼š\n"
            f"{guidelines}\n"
        )

    # ------------------------------------------------------------------
    # ğŸŒŸ å°è¯´é›†ä¸Šä¸‹æ–‡é‡ç½® (Novella Collection Context Reset)
    # ------------------------------------------------------------------

    def reset_context(self) -> None:
        """å¼ºåˆ¶é‡ç½®æ»‘åŠ¨çª—å£ï¼Œç”¨äºå°è¯´é›†ä¸­çš„æ–°æ•…äº‹"""
        self._prev_characters = []
        self._prev_tail_entries = []
        self._local_session_cast = {}
        logger.info("â™»ï¸ æ£€æµ‹åˆ°æ•…äº‹è¾¹ç•Œï¼Œå¯¼æ¼”å¼•æ“å·²é‡ç½®ä¸Šä¸‹æ–‡ã€‚")

    def _test_api_connection(self):
        """æµ‹è¯• LLM API æœåŠ¡è¿æ¥"""
        if not self.api_key:
            logger.warning("âš ï¸ æœªè®¾ç½® API Keyï¼Œæ™ºèƒ½é…éŸ³æ¨¡å¼å°†æ— æ³•ä½¿ç”¨å¤§æ¨¡å‹æœåŠ¡ã€‚")
            return False
        try:
            self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "è¯·å›å¤\u201cè¿æ¥æ­£å¸¸\u201då››ä¸ªå­—ã€‚"}],
                max_tokens=16,
            )
            logger.info("âœ… LLM API æœåŠ¡è¿æ¥æ­£å¸¸")
            return True
        except Exception as e:
            logger.warning(f"âŒ æ— æ³•è¿æ¥åˆ° LLM API æœåŠ¡: {e}")
            return False
    
    def _chunk_text_for_llm(self, text: str, max_length: int = 8000) -> List[str]:
        """ğŸŒŸ é˜²æ­¢ç« èŠ‚è¿‡é•¿ï¼ŒæŒ‰æ®µè½åˆ‡åˆ†ä¸ºå®‰å…¨å¤§å°ç»™ LLM å¤„ç†
        
        è™½ç„¶ä¸Šä¸‹æ–‡çª—å£ 1Mï¼Œä½†è¾“å‡ºé™åˆ¶ 32K tokenï¼Œä¸ºé˜²æ­¢ JSON è†¨èƒ€æˆªæ–­ï¼Œ
        å»ºè®®å•å— 8000 å­—ç¬¦ã€‚è¶…è¿‡ max_length çš„ç« èŠ‚ä¼šå°½é‡åˆ†æˆå¤§å°ç›¸è¿‘çš„å‡ éƒ¨åˆ†ï¼Œ
        é¿å…å‡ºç°ä¸€éƒ¨åˆ† 7800 å­—è€Œå¦ä¸€éƒ¨åˆ†åªæœ‰ 800 å­—çš„ä¸å‡åŒ€åˆ‡å‰²ã€‚
        """
        total_len = len(text)
        if total_len <= max_length:
            return [text] if text.strip() else []

        # è®¡ç®—éœ€è¦å‡ å—æ‰èƒ½è®©æ¯å—å¤§å°å°½é‡å‡åŒ€
        num_parts = (total_len + max_length - 1) // max_length
        target_size = min(total_len // num_parts, max_length)

        paragraphs = text.split('\n')
        chunks, current_chunk = [], ""
        for para in paragraphs:
            if not para.strip():
                continue
            if len(current_chunk) + len(para) > target_size and current_chunk:
                chunks.append(current_chunk)
                current_chunk = para + "\n"
            else:
                current_chunk += para + "\n"
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    def verify_integrity(self, original_text: str, script_list: List[Dict]) -> bool:
        """ğŸŒŸ å†…å®¹å®Œæ•´æ€§å…¨é¢ç›‘æ§ï¼šå¯¹æ¯”åŸæ–‡ä¸å‰§æœ¬å­—æ•°ï¼Œåˆ†çº§é¢„è­¦

        åˆ†çº§ç­–ç•¥ï¼š
        - ä¿ç•™ç‡ < 90%ï¼šä¸¥é‡ä¸¢å¤±ï¼Œè®°å½•è¯¦ç»†å·®å¼‚ï¼Œè¿”å› Falseï¼ˆè§¦å‘æ—ç™½å›é€€ï¼‰
        - ä¿ç•™ç‡ < 99%ï¼šè½»å¾®åå·®ï¼Œè®°å½•è¯¦ç»†å·®å¼‚åˆ°æ—¥å¿—ï¼Œè¿”å› True
        - ä¿ç•™ç‡ >= 99%ï¼šå®Œå…¨è¾¾æ ‡ï¼Œé™é»˜é€šè¿‡

        Args:
            original_text: åŸå§‹è¾“å…¥æ–‡æœ¬
            script_list: LLM è§£æåçš„å‰§æœ¬åˆ—è¡¨

        Returns:
            True è¡¨ç¤ºå†…å®¹å®Œæ•´æ€§è¾¾æ ‡ï¼ŒFalse è¡¨ç¤ºå†…å®¹ä¸¢å¤±ä¸¥é‡
        """
        if not original_text or not script_list:
            return True
        content_text = "".join([item.get("content", "") for item in script_list])
        original_len = len(original_text.strip())
        if original_len == 0:
            return True
        content_len = len(content_text)
        ratio = content_len / original_len
        diff_chars = original_len - content_len

        if ratio < 0.9:
            logger.error(
                f"ğŸš¨ å†…å®¹ä¸¢å¤±ä¸¥é‡ï¼åŸæ–‡{original_len}å­—ï¼Œ"
                f"è§£æåä»…{content_len}å­— (ä¿ç•™ç‡{ratio:.1%})"
            )
            logger.error(
                f"ğŸ“Š è¯¦ç»†å·®å¼‚: åŸæ–‡å­—æ•°={original_len}, å‰§æœ¬å­—æ•°={content_len}, "
                f"ç¼ºå¤±å­—æ•°={diff_chars}, ä¿ç•™ç‡={ratio:.2%}"
            )
            self._log_content_diff(original_text.strip(), content_text)
            return False

        if ratio < 0.99:
            logger.warning(
                f"âš ï¸ å†…å®¹å·®å¼‚æ£€æµ‹: åŸæ–‡{original_len}å­—ï¼Œå‰§æœ¬{content_len}å­— "
                f"(ä¿ç•™ç‡{ratio:.2%}, ç¼ºå¤±{diff_chars}å­—)"
            )
            self._log_content_diff(original_text.strip(), content_text)

        logger.info(f"âœ… å†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡ (ä¿ç•™ç‡{ratio:.1%})")
        return True

    def _log_content_diff(self, original_text: str, script_text: str) -> None:
        """å°†åŸæ–‡ä¸å‰§æœ¬çš„æ®µè½çº§å·®å¼‚å†™å…¥æ—¥å¿—ï¼Œä¾¿äºå®šä½ä¸¢å¤±å†…å®¹ã€‚"""
        orig_paras = [p.strip() for p in original_text.split('\n') if p.strip()]
        if not orig_paras:
            return
        missing_paras = []
        for i, para in enumerate(orig_paras):
            check_prefix = para[:20] if len(para) > 20 else para
            if check_prefix and check_prefix not in script_text:
                missing_paras.append((i + 1, para[:80]))
        if missing_paras:
            logger.warning(f"ğŸ“ ç–‘ä¼¼ç¼ºå¤±æ®µè½ ({len(missing_paras)}/{len(orig_paras)}æ®µ):")
            for para_num, preview in missing_paras[:10]:
                logger.warning(f"   ç¬¬{para_num}æ®µ: {preview}...")
            if len(missing_paras) > 10:
                logger.warning(f"   ... åŠå…¶ä½™ {len(missing_paras) - 10} æ®µ")
    
    def generate_pure_narrator_script(self, text: str, chapter_prefix: str = "chunk") -> List[Dict]:
        """
        çº¯å‡€æ—ç™½æ¨¡å¼ä¸“ç”¨çš„å‰§æœ¬ç”Ÿæˆå™¨ï¼ˆç»•è¿‡LLMï¼Œç§’çº§ç”Ÿæˆï¼Œ100%å¿ å®åŸè‘—ï¼‰
        çº¯å‡€æ—ç™½æ¨¡å¼ä¸‹ï¼Œåˆ‡ç‰‡é•¿åº¦æ”¾å®½åˆ° 100 å­—å·¦å³ï¼Œå‡å°‘åˆ‡ç‰‡æ•°é‡ï¼Œæå‡æœ—è¯»æµç•…åº¦ã€‚
        """
        micro_script = []
        chunk_id = 1

        # ğŸŒŸ çº¯å‡€æ—ç™½æ¨¡å¼ä¸‹ï¼Œåˆ‡ç‰‡ä¸Šé™æ”¾å®½åˆ° ~100 å­—
        pure_chunk_limit = self.pure_narrator_chunk_limit

        # 1. æŒ‰æ®µè½åˆ‡åˆ†
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]

        for p_idx, para in enumerate(paragraphs):
            # 2. æŒ‰é•¿å¥æ ‡ç‚¹åˆ‡åˆ†ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›.!?;])', para)

            temp_sentence = ""
            for part in sentences:
                if not part.strip() and not re.match(r'[ã€‚ï¼ï¼Ÿï¼›.!?;]', part):
                    continue

                if re.match(r'^[ã€‚ï¼ï¼Ÿï¼›.!?;]$', part.strip()):
                    temp_sentence += part

                    # 3. å¦‚æœå•å¥ä»ç„¶è¶…é•¿ï¼Œå¯åŠ¨é€—å·/é¡¿å·çš„æ¬¡çº§åˆ‡åˆ†
                    if len(temp_sentence) > pure_chunk_limit:
                        sub_parts = re.split(r'([ï¼Œã€ï¼š,:])', temp_sentence)
                        sub_temp = ""
                        for sub in sub_parts:
                            if re.match(r'^[ï¼Œã€ï¼š,:]$', sub):
                                sub_temp += sub
                                pause = self._calculate_pause(sub_temp, False)
                                micro_script.append({
                                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                                    "type": "narration",
                                    "speaker": "narrator",
                                    "gender": "male",
                                    "emotion": "å¹³é™",
                                    "content": sub_temp.strip(),
                                    "pause_ms": pause
                                })
                                chunk_id += 1
                                sub_temp = ""
                            else:
                                sub_temp += sub
                        if sub_temp.strip():
                            pause = self._calculate_pause(sub_temp, p_idx == len(paragraphs)-1)
                            micro_script.append({
                                "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                                "type": "narration",
                                "speaker": "narrator",
                                "gender": "male",
                                "emotion": "å¹³é™",
                                "content": sub_temp.strip(),
                                "pause_ms": pause
                            })
                            chunk_id += 1
                    else:
                        # æ­£å¸¸é•¿åº¦çš„å¥å­ç›´æ¥æ¨å…¥
                        pause = self._calculate_pause(temp_sentence, p_idx == len(paragraphs)-1)
                        micro_script.append({
                            "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                            "type": "narration",
                            "speaker": "narrator",
                            "gender": "male",
                            "emotion": "å¹³é™",
                            "content": temp_sentence.strip(),
                            "pause_ms": pause
                        })
                        chunk_id += 1
                    temp_sentence = ""
                else:
                    temp_sentence += part

            # å¤„ç†æ®µè½æœ«å°¾æ²¡æœ‰æ ‡ç‚¹çš„æ®‹ç•™éƒ¨åˆ†
            if temp_sentence.strip():
                pause = self._calculate_pause(temp_sentence, p_idx == len(paragraphs)-1)
                micro_script.append({
                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                    "type": "narration",
                    "speaker": "narrator",
                    "gender": "male",
                    "emotion": "å¹³é™",
                    "content": temp_sentence.strip(),
                    "pause_ms": pause
                })
                chunk_id += 1

        return micro_script

    def parse_and_micro_chunk(self, text: str, chapter_prefix: str = "chunk", max_length: int = 8000) -> List[Dict]:
        """å®è§‚å‰§æœ¬è§£æ -> è‡ªåŠ¨å±•å¼€ä¸ºå¾®åˆ‡ç‰‡å‰§æœ¬
        
        Args:
            text: å¾…å¤„ç†çš„ç« èŠ‚æ–‡æœ¬
            chapter_prefix: ç« èŠ‚åç§°å‰ç¼€ï¼Œç”¨äºé¿å…æ–‡ä»¶åå†²çª
            max_length: LLM å•æ¬¡å¤„ç†çš„æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ï¼Œé»˜è®¤8000
        """
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå®è§‚å‰§æœ¬
        macro_script = self.parse_text_to_script(text, max_length=max_length)

        # ğŸ›¡ï¸ å‰§æœ¬ç›‘æ§ï¼šå†…å®¹å·®å¼‚è¶…è¿‡90%æ—¶ï¼Œè‡ªåŠ¨å›é€€æ—ç™½æ¨¡å¼æ¸²æŸ“åŸæ–‡
        content_text = "".join(item.get("content", "") for item in macro_script)
        original_len = len(text.strip())
        if original_len > 0:
            ratio = len(content_text) / original_len
            if ratio < 0.9:
                logger.warning(
                    f"ğŸ›¡ï¸ å‰§æœ¬å†…å®¹ä¿ç•™ç‡è¿‡ä½ ({ratio:.1%})ï¼Œ"
                    f"è‡ªåŠ¨åˆ‡æ¢æ—ç™½æ¨¡å¼æ¸²æŸ“åŸæ–‡: {chapter_prefix}"
                )
                return self.generate_pure_narrator_script(text, chapter_prefix=chapter_prefix)

        micro_script = []
        chunk_id = 1
        
        # é€‚å½“æ”¾å®½å¾®åˆ‡ç‰‡çº¢çº¿ï¼Œé¿å…æ­£å¸¸å¥å­è¢«æ— æ•…åˆ‡æ–­
        smart_chunk_limit = max(self.max_chars_per_chunk, 150) # ğŸ¯ ä¿®æ”¹ç‚¹ï¼šä» 90 æ”¹ä¸º 150
        
        for unit in macro_script:
            content = unit.get("content", "")
            if not content or not content.strip():
                continue

            # ğŸŒŸ ä¿®å¤ï¼šå®æ–½æ™ºèƒ½å¾®åˆ‡ç‰‡ï¼Œä¼˜å…ˆæŒ‰å¤§æ ‡ç‚¹åˆ‡åˆ†
            raw_sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›.!?;])', content)
            chunks = []
            temp = ""
            for part in raw_sentences:
                if not part.strip():
                    continue
                if re.match(r'^[ã€‚ï¼ï¼Ÿï¼›.!?;]$', part.strip()):
                    temp += part
                    # å¦‚æœè¿™å¥é•¿åº¦æ­£å¸¸ï¼Œç›´æ¥åŠ å…¥ï¼ˆä¸å†è¢«é€—å·åˆ‡ç¢ï¼‰
                    if len(temp) <= smart_chunk_limit:
                        chunks.append(temp)
                        temp = ""
                    else:
                        # ğŸš¨ åªæœ‰å½“å•å¥è¶…é•¿æ—¶ï¼Œæ‰å¯åŠ¨é€—å·/é¡¿å·çš„æ¬¡çº§åˆ‡åˆ†
                        sub_parts = re.split(r'([ï¼Œã€ï¼š,:])', temp)
                        sub_temp = ""
                        for sub in sub_parts:
                            if re.match(r'^[ï¼Œã€ï¼š,:]$', sub):
                                sub_temp += sub
                                chunks.append(sub_temp)
                                sub_temp = ""
                            else:
                                sub_temp += sub
                        if sub_temp:
                            chunks.append(sub_temp)
                        temp = ""
                else:
                    temp += part
            if temp: chunks.append(temp)
            
            # æ¸…ç†ç©ºå—å¹¶è®¡ç®—åœé¡¿
            valid_chunks = [c.strip() for c in chunks if c.strip()]

            # ğŸŒŸ å…œåº•é€»è¾‘ï¼šå¦‚æœæ­£åˆ™åˆ‡åˆ†åæ— æœ‰æ•ˆå—ï¼ŒæŒ‰ç¡¬åˆ‡
            if not valid_chunks and content.strip():
                hard_cut_chunk_size = smart_chunk_limit
                stripped = content.strip()
                valid_chunks = [
                    stripped[i:i + hard_cut_chunk_size]
                    for i in range(0, len(stripped), hard_cut_chunk_size)
                ]
                logger.warning(
                    f"âš ï¸ æ­£åˆ™åˆ‡åˆ†æ— ç»“æœï¼Œå·²æŒ‰æ¯{hard_cut_chunk_size}å­—ç¡¬åˆ‡: "
                    f"'{content[:30]}...'"
                )

            for idx, chunk in enumerate(valid_chunks):
                is_para_end = (idx == len(valid_chunks) - 1)
                pause_ms = self._calculate_pause(chunk, is_para_end)
                
                # ğŸŒŸ ä¿®å¤ï¼šå°†ç« èŠ‚åç§°å‰ç¼€åŠ å…¥IDï¼Œæœç»æ–‡ä»¶è¦†ç›–ï¼
                micro_script.append({
                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                    "type": unit["type"],
                    "speaker": unit["speaker"],
                    "gender": unit.get("gender", "male"),
                    "content": chunk,
                    "pause_ms": pause_ms
                })
                chunk_id += 1
                
        return micro_script

    def _calculate_pause(self, chunk_text: str, is_para_end: bool) -> int:
        """æå‰è®¡ç®—å¥½ç‰©ç†åœé¡¿æ—¶é—´"""
        if is_para_end: return 1000
        if chunk_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')): return 600
        elif chunk_text.endswith(('ï¼›', ';')): return 400
        elif chunk_text.endswith(('ï¼Œ', 'ã€', ',', 'ï¼š', ':')): return 250
        else: return 100

    @staticmethod
    def _normalize_text(text: str) -> str:
        """å°†æ•°å­—å’Œå¸¸è§ç¬¦å·è½¬æ¢ä¸ºä¸­æ–‡å¯è¯»å½¢å¼ï¼Œé˜²æ­¢ TTS è¯¯è¯»ã€‚

        é‡‡ç”¨é€å­—è½¬æ¢ç­–ç•¥ï¼Œç¡®ä¿ TTS æœ—è¯»ä¸€è‡´æ€§ã€‚

        Examples:
            "10%" -> "ç™¾åˆ†ä¹‹ä¸€é›¶"
            "100" -> "ä¸€é›¶é›¶"
            "3.14" -> "ä¸‰ç‚¹ä¸€å››"
        """
        _DIGIT_MAP = {
            '0': 'é›¶', '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››',
            '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹',
        }

        def _digits_to_chinese(m: re.Match) -> str:
            """Convert a matched digit string to simple Chinese reading."""
            s = m.group(0)
            return ''.join(_DIGIT_MAP.get(c, c) for c in s)

        # ç™¾åˆ†å·ï¼š10% -> ç™¾åˆ†ä¹‹å, 12.5% -> ç™¾åˆ†ä¹‹ä¸€äºŒç‚¹äº”
        def _percent_repl(m: re.Match) -> str:
            num_str = m.group(1)
            if '.' in num_str:
                int_part, dec_part = num_str.split('.', 1)
                cn_int = ''.join(_DIGIT_MAP.get(c, c) for c in int_part)
                cn_dec = ''.join(_DIGIT_MAP.get(c, c) for c in dec_part)
                return f'ç™¾åˆ†ä¹‹{cn_int}ç‚¹{cn_dec}'
            cn = ''.join(_DIGIT_MAP.get(c, c) for c in num_str)
            return f'ç™¾åˆ†ä¹‹{cn}'

        text = re.sub(r'(\d+(?:\.\d+)?)%', _percent_repl, text)

        # å°æ•°ï¼š3.14 -> ä¸‰ç‚¹ä¸€å››
        def _decimal_repl(m: re.Match) -> str:
            integer_part = m.group(1)
            decimal_part = m.group(2)
            cn_int = ''.join(_DIGIT_MAP.get(c, c) for c in integer_part)
            cn_dec = ''.join(_DIGIT_MAP.get(c, c) for c in decimal_part)
            return f'{cn_int}ç‚¹{cn_dec}'

        text = re.sub(r'(\d+)\.(\d+)', _decimal_repl, text)

        # çº¯æ•´æ•°åºåˆ—ï¼šè¿ç»­æ•°å­— -> é€å­—è½¬æ¢
        text = re.sub(r'\d+', _digits_to_chinese, text)

        return text

    def parse_text_to_script(self, text: str, max_length: int = 8000) -> List[Dict]:
        """é˜¶æ®µä¸€ï¼šå®è§‚å‰§æœ¬è§£æ (Qwen-Flash é«˜æ•ˆå¹¶å‘ç‰ˆ)

        è™½ç„¶ Qwen-Flash æ‹¥æœ‰ 1M token ä¸Šä¸‹æ–‡ï¼Œä½†è¾“å‡ºé™åˆ¶ 32K tokenã€‚
        ä¸ºé˜²æ­¢ JSON è†¨èƒ€æˆªæ–­ï¼Œå°†åˆ‡ç‰‡é•¿åº¦è°ƒæ•´ä¸º 8000 å­—ç¬¦ã€‚

        Args:
            text: å¾…å¤„ç†çš„ç« èŠ‚æ–‡æœ¬
            max_length: LLM å•æ¬¡å¤„ç†çš„æœ€å¤§å­—ç¬¦æ•°ä¸Šé™ï¼Œé»˜è®¤8000
        """
        logger.info(f"ğŸš€ å¯åŠ¨ {self.model_name} å‰§æœ¬è§£æï¼Œå½“å‰ç« èŠ‚å­—æ•°: {len(text)}")

        # ğŸŒŸ Qwen-Flash æ‹¥æœ‰ 1M è¶…å¤§ä¸Šä¸‹æ–‡ï¼Œæ•´ç« ç›´å‡ºï¼Œä»…è¶…é•¿ç« èŠ‚æ‰åˆ‡åˆ†
        text_chunks = self._chunk_text_for_llm(text, max_length=max_length)
        full_script = []
        
        for i, chunk in enumerate(text_chunks):
            logger.info(f"   ğŸ§  æ­£åœ¨è§£æå‰§æƒ…ç‰‡æ®µ {i+1}/{len(text_chunks)}...")
            
            # Build context from previous chunk
            context_parts: List[str] = []
            if self._prev_characters:
                context_parts.append(
                    "å‰ä¸€æ®µå‡ºåœºè§’è‰²: " + ", ".join(self._prev_characters)
                )
            if self._prev_tail_entries:
                try:
                    tail_json = json.dumps(
                        self._prev_tail_entries, ensure_ascii=False
                    )
                    context_parts.append(
                        "\nPrevious section ended with:\n" + tail_json
                    )
                except Exception:
                    pass
            
            chunk_script = self._request_llm(chunk, context="\n".join(context_parts) if context_parts else None)
            
            # Update sliding window state
            if chunk_script:
                speakers = {
                    e.get("speaker")
                    for e in chunk_script
                    if e.get("speaker") and e.get("speaker") != "narrator"
                }
                if speakers:
                    self._prev_characters = list(speakers)
                self._prev_tail_entries = chunk_script[-3:]

                # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§é˜²æŠ¤ï¼šè®°å½•è§’è‰²çš„éŸ³è‰²æè¿°åˆ°å±€éƒ¨ä¼šè¯è§’è‰²è¡¨
                for e in chunk_script:
                    speaker = e.get("speaker")
                    emotion = e.get("emotion", "")
                    if speaker and speaker != "narrator" and emotion:
                        if speaker not in self._local_session_cast:
                            self._local_session_cast[speaker] = emotion

                # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§æŒä¹…åŒ–ï¼šå°†æ–°è§’è‰²éŸ³è‰²å†™å…¥ JSON è§’è‰²åº“
                self._update_cast_db(chunk_script)
            
            full_script.extend(chunk_script)

            # äº‘ç«¯ API çš„é¢‘ç‡é™åˆ¶ç”± _request_llm å†…éƒ¨çš„ 429 é€€é¿é€»è¾‘è‡ªåŠ¨æ§åˆ¶ï¼Œæ— éœ€äººä¸ºèŠ‚æµ
        
        # ğŸŒŸ ä¼˜åŒ–ï¼šç§»é™¤ merge_consecutive_narrators è°ƒç”¨ã€‚
        # å› ä¸º parse_and_micro_chunk ä¼šå¯¹ç»“æœè¿›è¡Œä¸¥æ ¼çš„ 60 å­—å¾®åˆ‡ç‰‡ï¼Œ
        # åˆå¹¶åçš„ 800 å­—é•¿æ–‡æœ¬ä¼šè¢«ç«‹å³ç¢¾ç¢ï¼Œå±äºæ— è°“çš„ç®—åŠ›æµªè´¹ã€‚
        
        # å¦‚æœè§£æç»“æœä¸ºç©ºï¼Œç›´æ¥æŠ¥é”™é€€å‡º
        if not full_script or len(full_script) == 0:
            raise RuntimeError("âŒ å‰§æœ¬è§£æç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡æœ¬å’Œå¤§æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸ã€‚")

        # ğŸŒŸ å†…å®¹å®Œæ•´æ€§å®ˆé—¨å‘˜ï¼šæ£€æµ‹ LLM æ˜¯å¦ä¸¥é‡åˆ èŠ‚å†…å®¹
        if not self.verify_integrity(text, full_script):
            logger.warning("âš ï¸ å†…å®¹å®Œæ•´æ€§æ ¡éªŒæœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥å¤§æ¨¡å‹è¾“å‡ºè´¨é‡ã€‚")
            logger.error("âŒ å†…å®¹å®Œæ•´æ€§ä½ã€‚å»ºè®®é™ä½ parse_and_micro_chunk() çš„ max_length å‚æ•°åé‡è¯•ã€‚")
            
        return full_script
    
    def generate_chapter_recap(self, prev_chapter_text: str) -> str:
        """
        ğŸŒŸ å‰æƒ…æ‘˜è¦å¼•æ“ (Qwen-Flash è¶…å¤§ä¸Šä¸‹æ–‡ç‰ˆ)
        åˆ©ç”¨ Qwen-Flash çš„ 1M è¶…å¤§ä¸Šä¸‹æ–‡ï¼Œç›´æ¥æ•´ç« ä¼ å…¥ç”Ÿæˆæ‘˜è¦ï¼Œ
        æ— éœ€ Map-Reduce åˆ†å—å¤„ç†ã€‚
        """
        # 1. åŸºç¡€æ¸…ç†
        text = prev_chapter_text.strip()
        if not text:
            return ""

        logger.info(f"ğŸš€ å¯åŠ¨ {self.model_name} å‰æƒ…æ‘˜è¦ç”Ÿæˆï¼Œä¸Šä¸€ç« å­—æ•°: {len(text)}")

        # ç›´æ¥ç”Ÿæˆç»ˆææ‘˜è¦ + æ‚¬å¿µé’©å­ï¼ˆQwen 1M ä¸Šä¸‹æ–‡è¶³ä»¥å®¹çº³æ•´ç« å†…å®¹ï¼‰
        reduce_prompt = (
            'ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æœ‰å£°ä¹¦å‰§æœ¬ç¼–è¾‘å’Œæ‚¬ç–‘å¤§å¸ˆã€‚'
            'è¯·æ ¹æ®æä¾›çš„ä¸Šä¸€ç« å†…å®¹ï¼Œå†™ä¸€æ®µä¸è¶…è¿‡100å­—çš„\u201cå‰æƒ…æ‘˜è¦\u201dã€‚'
            'ç»å¯¹çºªå¾‹ï¼š'
            '1. è¯­è¨€å¿…é¡»é«˜åº¦å‡ç»ƒï¼Œå…·æœ‰ç¾å‰§ç‰‡å¤´çš„ç”µå½±æ„Ÿï¼ˆ\u201cPreviously on...\u201dçš„é£æ ¼ï¼‰ã€‚'
            '2. åªä¿ç•™æœ€å…·å¼ åŠ›çš„å‰§æƒ…çŸ›ç›¾ã€‚'
            '3. æœ€åä¸€å¥å¿…é¡»æ˜¯ä¸€ä¸ªå¼•å‡ºä¸‹ä¸€ç« çš„\u201cæ‚¬å¿µé’©å­\u201dã€‚'
            '4. ç»å¯¹ä¸è¦è¾“å‡º\u201cå‰æƒ…æè¦ï¼š\u201dè¿™æ ·çš„æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºæ­£æ–‡ã€‚'
        )

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": reduce_prompt},
                    {"role": "user", "content": f"ä¸Šä¸€ç« å†…å®¹ï¼š\n{text}"}
                ],
                stream=False,
                temperature=0.5,
                top_p=0.8,
                max_tokens=8192,
            )
            recap_result = response.choices[0].message.content.strip()

            # æ¸…ç†å¤§æ¨¡å‹å¯èƒ½è¿è§„åŠ ä¸Šçš„å‰ç¼€
            recap_result = re.sub(r'^(å‰æƒ…æè¦|å‰æƒ…æ‘˜è¦|å›é¡¾|æ‘˜è¦)[:ï¼š]\s*', '', recap_result)
            return recap_result
        except Exception as e:
            logger.error(f"ç»ˆææ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _request_llm(self, text_chunk: str, *, context: Optional[str] = None) -> List[Dict]:
        """å‘ Qwen API å‘é€å•ä¸ªæ–‡æœ¬å—è¯·æ±‚

        Args:
            text_chunk: The raw text to convert into a script.
            context: Optional sliding-window context from the previous chunk
                     (character list + tail entries) to maintain consistency.
        """
        # ğŸŒŸ é˜²å¹»è§‰åŠ å›ºï¼šå®šä¹‰ Qwen3-TTS å®˜æ–¹æ”¯æŒçš„æ„Ÿæƒ…å­é›†ï¼Œé˜²æ­¢æ¨¡å‹ä¹±å†™
        EMOTION_SET = "å¹³é™, æ„¤æ€’, æ‚²ä¼¤, å–œæ‚¦, ææƒ§, æƒŠè®¶, æ²§æ¡‘, æŸ”å’Œ, æ¿€åŠ¨, å˜²è®½, å“½å’½, å†°å†·, ç‹‚å–œ"

        # ğŸŒŸ é˜²å¹»è§‰åŠ å›ºï¼šé«˜ç²¾åº¦æœ‰å£°ä¹¦å‰§æœ¬è½¬æ¢æ¥å£ System Prompt
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„æœ‰å£°ä¹¦å‰§æœ¬è½¬æ¢æ¥å£ã€‚
ä»»åŠ¡ï¼šå°†è¾“å…¥æ–‡æœ¬é€å¥è§£æä¸º JSON æ•°ç»„æ ¼å¼ã€‚
æ ¸å¿ƒè§„åˆ™ï¼š
1. å®Œæ•´æ€§ï¼šåŸæ–‡å¿…é¡»è¢«å®Œå…¨ä¿ç•™ï¼Œä¸¥ç¦åˆ å‡ã€‚
2. è¿è´¯æ€§åŸåˆ™ï¼ˆæœ€æ ¸å¿ƒæŒ‡ä»¤ï¼‰ï¼šä¸ºä¿è¯æœ‰å£°ä¹¦æœ—è¯»çš„æµç•…æ„Ÿï¼Œå¯¹äºåŒä¸€è§’è‰²çš„è¿ç»­å¤šå¥å°è¯ï¼Œæˆ–è¿ç»­çš„ä¸€æ•´æ®µæ—ç™½ï¼Œåœ¨æ€»å­—æ•°ä¸è¶…è¿‡ 150 å­—çš„æƒ…å†µä¸‹ï¼Œå¿…é¡»åˆå¹¶åœ¨åŒä¸€ä¸ª JSON å¯¹è±¡å†…ï¼ç»å¯¹ä¸å…è®¸æŠŠä¸€ä¸ªè§’è‰²çš„ä¸€å¥å®Œæ•´çš„è¯åˆ‡ç¢ï¼
3. è¾¹ç•Œåˆ‡åˆ†ï¼šåªæœ‰å½“è¯´è¯äººå‘ç”Ÿæ”¹å˜ï¼ˆä¾‹å¦‚ä»è§’è‰²Aè½¬ä¸ºè§’è‰²Bï¼Œæˆ–è§’è‰²è½¬ä¸ºæ—ç™½ï¼‰ï¼Œæˆ–è€…å•æ¡æ–‡æœ¬é•¿åº¦è¶…è¿‡ 150 å­—æ—¶ï¼Œæ‰æ–°å»ºä¸€ä¸ª JSON å¯¹è±¡ã€‚
4. æ ¹èŠ‚ç‚¹çº¦æŸï¼šå¿…é¡»æ˜¯æ ‡å‡†çš„ JSON æ•°ç»„ï¼ˆä»¥ `[` å¼€å¤´ï¼‰ã€‚
5. å­—æ®µè¦æ±‚ï¼šåŒ…å« type, speaker, gender, emotion, content å­—æ®µã€‚

ã€ğŸš¨ é˜²æˆªæ–­æ­»äº¡çº¢çº¿ã€‘
è¯·ç§‰æŒæåº¦çš„è€å¿ƒï¼Œé€å­—é€å¥è§£æç›´åˆ°æœ€åï¼Œåˆ‡å¿Œè¿‡åº¦ç¢ç‰‡åŒ–ï¼
"""

        # ğŸŒŸ ä¼˜åŒ– Few-Shotï¼Œç¤ºèŒƒæ­£ç¡®çš„åˆå¹¶ä¿ç•™è¡Œä¸º
        one_shot_example = """
ã€è¾“å…¥ã€‘ï¼š
"ä½ å¥½å•Šå¹´è½»äººï¼Œè¿™æµ·é£å¯çœŸå¤Ÿå†·çš„ã€‚"è€æ¸”å¤«ç´§ç´§è£¹äº†è£¹å¤§è¡£ï¼Œå¹äº†å£æ°”ï¼Œ"æ˜¨æ™šçš„æš´é£é›ªå·®ç‚¹æŠŠæˆ‘çš„èˆ¹ç»™æ€ç¿»äº†ã€‚"
ã€è¾“å‡ºã€‘ï¼š
[
  {"type": "dialogue", "speaker": "è€æ¸”å¤«", "gender": "male", "emotion": "æ²§æ¡‘", "content": "ä½ å¥½å•Šå¹´è½»äººï¼Œè¿™æµ·é£å¯çœŸå¤Ÿå†·çš„ã€‚"},
  {"type": "narration", "speaker": "narrator", "gender": "male", "emotion": "å¹³é™", "content": "è€æ¸”å¤«ç´§ç´§è£¹äº†è£¹å¤§è¡£ï¼Œå¹äº†å£æ°”ï¼Œ"},
  {"type": "dialogue", "speaker": "è€æ¸”å¤«", "gender": "male", "emotion": "åæ€•", "content": "æ˜¨æ™šçš„æš´é£é›ªå·®ç‚¹æŠŠæˆ‘çš„èˆ¹ç»™æ€ç¿»äº†ã€‚"}
]
"""

        # ğŸŒŸ å…¨å±€é€‰è§’çºªå¾‹æ³¨å…¥ï¼šå¦‚æœæœ‰å¤–è„‘æä¾›çš„è§’è‰²ç™½åå•ï¼Œè¿½åŠ åˆ° system_prompt
        if self.global_cast:
            cast_names = list(self.global_cast.keys())
            cast_info_parts = []
            for name, info in self.global_cast.items():
                if isinstance(info, dict):
                    g = info.get("gender", "unknown")
                    cast_info_parts.append(f'"{name}"(gender={g})')
                else:
                    cast_info_parts.append(f'"{name}"')
            cast_listing = ", ".join(cast_info_parts)
            system_prompt += f"""

        ã€å…¨å±€é€‰è§’çºªå¾‹ï¼ˆCast Whitelistï¼‰ã€‘
        - ä»¥ä¸‹æ˜¯æœ¬ä¹¦çš„å®˜æ–¹è§’è‰²åå•ï¼ˆæ ‡å‡†åï¼‰ï¼š{cast_listing}
        - ä½ åœ¨ speaker å­—æ®µä¸­ä½¿ç”¨çš„è§’è‰²åï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°æ ‡å‡†åï¼
        - ä¸¥ç¦è‡ªè¡Œå‘æ˜æˆ–ä½¿ç”¨ä»»ä½•ä¸åœ¨åå•ä¸­çš„è§’è‰²åï¼
        - å¦‚æœé‡åˆ°åå•å¤–çš„é¾™å¥—è§’è‰²ï¼Œç»Ÿä¸€ä½¿ç”¨ "è·¯äºº" ä½œä¸º speakerã€‚
        - å¦‚æœè§’è‰²ä¸åœ¨åå•ä¸­ï¼Œè¯·åœ¨è¯¥è§’è‰²çš„ emotion å­—æ®µä¸­é¢å¤–ç”Ÿæˆä¸€ä¸ª 10 è¯ä»¥å†…çš„è‹±æ–‡éŸ³è‰²æè¿°ï¼ˆå¦‚ï¼šA deep, husky voiceï¼‰ï¼Œä»¥ä¾¿ TTS å¼•æ“è¿›è¡ŒéŸ³è‰²è®¾è®¡ã€‚
        """

        # ğŸŒŸ Qwen3-TTS éŸ³è‰²æ˜ å°„æŒ‡å—æ³¨å…¥ï¼ˆåŠ¨æ€ä½¿ç”¨ VOICE_ARCHETYPESï¼‰
        system_prompt += self._get_archetype_prompt()

        # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§é˜²æŠ¤ï¼šæ³¨å…¥æŒä¹…åŒ–è§’è‰²éŸ³è‰²åº“ä¸­çš„å·²çŸ¥è§’è‰²
        if self.cast_profiles:
            known_cast_str = ", ".join(
                [f"{k}({v.get('gender', 'unknown')})" for k, v in self.cast_profiles.items()]
            )
            system_prompt += f"""

        ã€å·²çŸ¥è§’è‰²éŸ³è‰²åº“ï¼ˆPersistent Cast DBï¼‰ã€‘
        ä»¥ä¸‹è§’è‰²åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­å·²ç¡®å®šéŸ³è‰²ï¼Œè¯·ä¸¥æ ¼å¤ç”¨ï¼š{known_cast_str}
        """

        # ğŸŒŸ éŸ³è‰²ä¸€è‡´æ€§é˜²æŠ¤ï¼šæ³¨å…¥ä¸Šä¸€ chunk ä¸­å·²ç¡®å®šçš„éŸ³è‰²æè¿°
        if self._local_session_cast:
            cast_desc_parts = []
            for name, desc in self._local_session_cast.items():
                cast_desc_parts.append(f'"{name}": "{desc}"')
            cast_desc_listing = ", ".join(cast_desc_parts)
            system_prompt += f"""

        ã€è§’è‰²éŸ³è‰²é”å®šï¼ˆVoice Lockï¼‰ã€‘
        ä»¥ä¸‹è§’è‰²åœ¨å‰æ–‡ä¸­å·²ç¡®å®šéŸ³è‰²ï¼Œè¯·ä¸¥æ ¼å¤ç”¨ï¼Œç¦æ­¢æ›´æ”¹ï¼š
        {cast_desc_listing}
        """

        # ğŸŒŸ æ–‡æœ¬é¢„å¤„ç†ï¼šæ•°å­—/ç¬¦å·è§„èŒƒåŒ–
        text_chunk = self._normalize_text(text_chunk)

        # ğŸŒŸ é˜²å¹»è§‰åŠ å›ºï¼šå°† ASCII åŒå¼•å·æ›¿æ¢ä¸ºä¸­æ–‡åŒå¼•å·ï¼Œé¿å…ä¸ JSON ç»“æ„å†²çª
        # å…ˆå¤„ç†æˆå¯¹çš„ ASCII å¼•å·ï¼Œå†å°†å‰©ä½™çš„æ•£å¼•å·ç»Ÿä¸€æ›¿æ¢ä»¥æ¶ˆé™¤ JSON è§£æå¹²æ‰°
        text_chunk = re.sub(
            r'"([^"]*)"',
            lambda m: '\u201c' + m.group(1) + '\u201d',
            text_chunk,
        )
        text_chunk = text_chunk.replace('"', '\u2018')

        # ğŸŒŸ æ¨¡å‹çŠ¶æ€ç›‘æ§ä¸ Debug æç¤º
        input_len = len(text_chunk) + (len(context) if context else 0)
        logger.info(f"ğŸš€ æ¨¡å‹: {self.model_name} | å‘èµ·è¯·æ±‚ï¼Œä¼°è®¡ä¸Šä¸‹æ–‡é•¿åº¦: {input_len} å­—ç¬¦")

        # ğŸŒŸ Qwen API ä½¿ç”¨ 1M ä¸Šä¸‹æ–‡çª—å£ï¼Œæœ€å¤§è¾“å‡º 32K token

        # ğŸŒŸ é˜²å¹»è§‰åŠ å›ºï¼šç»“æ„åŒ– User Promptï¼ˆä½¿ç”¨æ¸©å’Œçš„ä»»åŠ¡æè¿°ï¼Œé¿å…è§¦å‘å†…å®¹å®‰å…¨è¿‡æ»¤ï¼‰
        user_content = "è¯·å°†ä»¥ä¸‹å°è¯´æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡† JSON æ•°ç»„æ ¼å¼ï¼ˆæœ€å¤–å±‚ä¸ºæ•°ç»„ï¼‰ï¼Œç”¨äºæœ‰å£°ä¹¦åˆ¶ä½œã€‚\n\n"

        if context:
            user_content += f"ä¸Šä¸‹æ–‡å‚è€ƒï¼š\n{context}\n\n"

        user_content += f"å¾…å¤„ç†åŸæ–‡ï¼š\n{text_chunk}"

        messages = [
            {"role": "system", "content": system_prompt + "\nç¤ºä¾‹å‚è€ƒï¼š" + one_shot_example},
            {"role": "user", "content": user_content}
        ]

        logger.info(f"ğŸš€ å‘èµ· {self.model_name} è§£æè¯·æ±‚ | åŸæ–‡å­—æ•°: {len(text_chunk)}")

        max_retries = 3

        for attempt in range(max_retries):
            try:
                # ğŸŒŸ ä¼˜åŒ–ï¼šä½¿ç”¨åŸç”Ÿçš„ OpenAI SDK å‘èµ·è¯·æ±‚
                completion = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    stream=True,
                    temperature=0.1,
                    max_tokens=32000,
                )

                full_content = ""

                # ğŸŒŸ ä¼˜åŒ–ï¼šä¼˜é›…çš„æµå¼è¯»å–ï¼Œæ²¡æœ‰ä»»ä½•é˜»ç¢é€Ÿåº¦çš„ sleep
                for chunk in completion:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, "content") and delta.content:
                        full_content += delta.content

                content = full_content.strip()

                # ğŸŒŸ æ¸…ç† Markdown æ ‡è®°
                content = content.replace('\t', ' ').replace('\r', '')
                content = re.sub(r'^```(?:json)?\s*', '', content, flags=re.IGNORECASE)
                content = re.sub(r'\s*```$', '', content)

                try:
                    script = json.loads(content)
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤æˆªæ–­çš„ JSON ...")
                    script = repair_json_array(content)
                    if script is None:
                        # ã€ç»ˆæé™çº§ 1ã€‘ï¼šJSONå½»åº•æŸåï¼Œç›´æ¥æ‹¿åŸæ–‡æœ¬åšæ—ç™½
                        logger.warning("âš ï¸ JSONå½»åº•æŸåï¼Œå¯ç”¨ç»ˆæé™çº§æ–¹æ¡ˆï¼šåŸæ–‡æœ¬ä½œä¸ºæ—ç™½ã€‚")
                        return self._validate_script_elements([
                            {"type": "narration", "speaker": "narrator", "content": text_chunk}
                        ])
                    return self._validate_script_elements(script)

                if isinstance(script, list):
                    return self._validate_script_elements(script)

                if isinstance(script, dict):
                    # å®¹é”™ 1: ç©ºå­—å…¸ {}
                    if not script:
                        logger.warning("âš ï¸ æ¨¡å‹è¿”å›äº†ç©ºå­—å…¸ï¼Œå¯ç”¨ç»ˆæé™çº§æ–¹æ¡ˆã€‚")
                        return self._validate_script_elements([
                            {"type": "narration", "speaker": "narrator", "content": text_chunk}
                        ])

                    # å®¹é”™ 2a: LLM è¿”å›äº† {"name": "...", "content": "..."} ç»“æ„
                    if "content" in script and "name" in script:
                        logger.warning("âš ï¸ æ£€æµ‹åˆ°éæ•°ç»„ç»“æ„ï¼ˆå« name/contentï¼‰ï¼Œæ­£åœ¨å°†å…¶è½¬æ¢ä¸ºå•æ¡æ—ç™½")
                        script = [{"type": "narration", "speaker": "narrator", "content": script["content"]}]
                        return self._validate_script_elements(script)
                    # å®¹é”™ 2b: LLM è¿”å›äº†å•ä¸ª JSON å¯¹è±¡ï¼ˆå¦‚ {"type": "narration", "speaker": "narrator", "content": "..."}ï¼‰
                    if "content" in script or "type" in script:
                        logger.warning("âš ï¸ æ¨¡å‹è¿”å›äº†å•ä¸ª JSON å¯¹è±¡è€Œéæ•°ç»„ï¼Œè‡ªåŠ¨ä½¿ç”¨åˆ—è¡¨åŒ…è£¹ä»¥æ¢å¤æµæ°´çº¿ã€‚")
                        return self._validate_script_elements([script])
                    # å®¹é”™ 3: LLM è¿”å›äº†åŒ…å«åˆ—è¡¨çš„å­—å…¸ (å¦‚ {"script": [...]})
                    for value in script.values():
                        if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
                            return self._validate_script_elements(value)
                    
                    # ã€ç»ˆæé™çº§ 2ã€‘ï¼šæ¨¡å‹è¿”å›äº†ç‰ˆæƒé¡µã€ä¹¦ç±å…ƒæ•°æ®ç­‰æ— æ³•è¯†åˆ«çš„å­—å…¸
                    logger.warning(f"âš ï¸ æ¨¡å‹è¿”å›äº†æ— æ³•è¯†åˆ«çš„å­—å…¸ç»“æ„ï¼ˆå¦‚ç‰ˆæƒä¿¡æ¯ï¼‰ï¼Œå¯ç”¨ç»ˆæé™çº§æ–¹æ¡ˆã€‚")
                    return self._validate_script_elements([
                        {"type": "narration", "speaker": "narrator", "content": text_chunk}
                    ])
                    
                # ã€ç»ˆæé™çº§ 3ã€‘ï¼šå¤§æ¨¡å‹è¿”å›äº†å­—ç¬¦ä¸²æˆ–æ•°å­—ç­‰å®Œå…¨ä¸æ˜¯å¯¹è±¡çš„æ ¼å¼
                logger.warning("âš ï¸ æ¨¡å‹è¿”å›äº†éé¢„æœŸç»“æ„ï¼Œå¯ç”¨ç»ˆæé™çº§æ–¹æ¡ˆã€‚")
                return self._validate_script_elements([
                    {"type": "narration", "speaker": "narrator", "content": text_chunk}
                ])

            except Exception as e:
                error_msg = str(e)
                # ğŸŒŸ ä¿®å¤ï¼šç²¾å‡†æ‹¦æˆªé˜¿é‡Œäº‘é£æ§ç³»ç»Ÿçš„ç‰¹æœ‰æŠ¥é”™
                if "inappropriate content" in error_msg or "Data inspection failed" in error_msg:
                    logger.error("ğŸš¨ è‡´å‘½æ‹¦æˆªï¼šè§¦å‘é˜¿é‡Œäº‘åº•çº¿å®‰å…¨é£æ§ï¼å†…å®¹æ¶‰å«Œæ•æ„Ÿã€‚")
                    logger.error("âš¡ æ”¾å¼ƒæ— æ„ä¹‰çš„é‡è¯•ï¼Œç¬é—´è§¦å‘ç»ˆæé™çº§æ–¹æ¡ˆï¼ˆå…¨é‡åŸæ–‡æœ¬è½¬æ—ç™½ï¼‰ï¼Œæ‹¯æ•‘æœ¬ç« èŠ‚ï¼")
                    return self._validate_script_elements([
                        {"type": "narration", "speaker": "narrator", "content": text_chunk}
                    ])
                
                # æ­£å¸¸çš„ç½‘ç»œæ³¢åŠ¨æˆ–è¶…æ—¶ï¼Œç»§ç»­é€€é¿é‡è¯•
                wait_time = 5 * (2 ** attempt)
                logger.warning(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ ({e})ï¼Œç­‰å¾… {wait_time}s åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})...")
                time.sleep(wait_time)
                continue

        raise RuntimeError("âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒQwen API è¯·æ±‚å½»åº•å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„ DASHSCOPE_API_KEY æ˜¯å¦æœ‰æ•ˆä»¥åŠè´¦æˆ·é¢åº¦æ˜¯å¦å……è¶³ã€‚")
    
    def _validate_script_elements(self, script: List[Dict]) -> List[Dict]:
        """éªŒè¯å¹¶ä¿®å¤è„šæœ¬å…ƒç´ ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ"""
        required_fields = ['type', 'speaker', 'content']
        validated_script = []
        
        for i, element in enumerate(script):
            # ç¡®ä¿æ˜¯å­—å…¸ç±»å‹
            if not isinstance(element, dict):
                logger.warning(f"âš ï¸ è„šæœ¬å…ƒç´  {i} ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡: {element}")
                continue
                
            # æ£€æŸ¥å¹¶è¡¥å……ç¼ºå¤±çš„å­—æ®µ
            fixed_element = element.copy()

            # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šå¦‚æœå¤§æ¨¡å‹æŠŠ content å†™æˆäº†æ•°ç»„ï¼Œå¼ºåˆ¶æ‹¼æˆå­—ç¬¦ä¸²
            if 'content' in fixed_element:
                if isinstance(fixed_element['content'], list):
                    fixed_element['content'] = "\n".join(str(x) for x in fixed_element['content'])
                elif not isinstance(fixed_element['content'], str):
                    fixed_element['content'] = str(fixed_element['content'])
            
            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            for field in required_fields:
                if field not in fixed_element:
                    if field == 'type':
                        fixed_element['type'] = 'narration'  # é»˜è®¤ä¸ºæ—ç™½
                    elif field == 'speaker':
                        fixed_element['speaker'] = 'narrator'  # é»˜è®¤è¯´è¯è€…
                    elif field == 'content':
                        fixed_element['content'] = ''  # ç©ºå†…å®¹
                    logger.warning(f"âš ï¸ è¡¥å……ç¼ºå¤±å­—æ®µ '{field}' åœ¨å…ƒç´  {i}: {element}")
            
            # å¼ºåŒ–ä¿®å¤é€»è¾‘ï¼šå¤„ç† None å€¼
            if fixed_element.get('speaker') is None:
                fixed_element['speaker'] = 'narrator'
                logger.warning(f"âš ï¸ ä¿®å¤ None å€¼å­—æ®µ 'speaker' åœ¨å…ƒç´  {i}")
            if fixed_element.get('gender') is None:
                fixed_element['gender'] = 'unknown'
                logger.warning(f"âš ï¸ ä¿®å¤ None/ç¼ºå¤±å­—æ®µ 'gender' åœ¨å…ƒç´  {i}")
            
            # ç¡®ä¿ gender å­—æ®µå­˜åœ¨ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰
            if 'gender' not in fixed_element:
                fixed_element['gender'] = 'unknown'
                logger.warning(f"âš ï¸ è¡¥å……ç¼ºå¤±å­—æ®µ 'gender' åœ¨å…ƒç´  {i}: {element}")
            
            # ç¡®ä¿ emotion å­—æ®µå­˜åœ¨
            if 'emotion' not in fixed_element:
                fixed_element['emotion'] = 'å¹³é™'

            # ğŸŒŸ éŸ³è‰²é˜²æŠ¤ï¼šå¦‚æœ emotion ä¸ºç©ºï¼Œä¸”è§’è‰²ä¸æ˜¯ narratorï¼Œ
            # æ ¹æ®æ€§åˆ«èµ‹äºˆ VOICE_ARCHETYPES ä¸­çš„é»˜è®¤éŸ³è‰²æè¿°ï¼Œé˜²æ­¢ TTS å‹åˆ¶å‡º"æœºæ¢°éŸ³"
            emotion_val = fixed_element.get('emotion', '')
            speaker_val = fixed_element.get('speaker', 'narrator')
            if speaker_val != 'narrator' and isinstance(emotion_val, str):
                stripped_emotion = emotion_val.strip()
                if not stripped_emotion:
                    gender_val = fixed_element.get('gender', 'unknown')
                    if gender_val == 'female':
                        default_desc = self.VOICE_ARCHETYPES.get("melancholic", "")
                    else:
                        default_desc = self.VOICE_ARCHETYPES.get("intellectual", "")
                    fixed_element['emotion'] = f"å¹³é™ ({default_desc})"
                    logger.warning(
                        f"âš ï¸ è§’è‰² '{speaker_val}' çš„ emotion ä¸ºç©ºï¼Œå·²è‡ªåŠ¨è¡¥å……é»˜è®¤éŸ³è‰²æè¿°"
                    )

            # ğŸŒŸ éŸ³è‰²å†²çªæ£€æµ‹ï¼šfemale è§’è‰²ä¸åº”ä½¿ç”¨ baritone/bass æè¿°
            gender = fixed_element.get('gender', 'unknown')
            emotion = fixed_element.get('emotion', '')
            if gender == 'female' and isinstance(emotion, str):
                emotion_lower = emotion.lower()
                if any(kw in emotion_lower for kw in ('baritone', 'bass', 'deep baritone')):
                    logger.warning(
                        f"âš ï¸ éŸ³è‰²å†²çªï¼šå¥³æ€§è§’è‰² '{fixed_element.get('speaker')}' "
                        f"çš„ emotion åŒ…å«ç”·æ€§éŸ³è‰²æè¿° '{emotion}'ï¼Œå·²è‡ªåŠ¨ä¿®æ­£"
                    )
                    fixed_element['emotion'] = re.sub(
                        r'\b(baritone|bass)\b', 'alto', emotion, flags=re.IGNORECASE
                    )
            
            validated_script.append(fixed_element)
            
        return validated_script
    

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    director = LLMScriptDirector()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
ç¬¬ä¸€ç«  å‡¯å¤«æ‹‰ç»´å…‹çš„é£é›ª

å¤œå¹•é™ä¸´ï¼Œæ¸¯å£çš„ç¯ç«å¼€å§‹é—ªçƒã€‚

"ä½ ç›¸ä¿¡å‘½è¿å—ï¼Ÿ"è€æ¸”å¤«è¯´é“ã€‚

å¹´è½»äººæ‘‡æ‘‡å¤´ï¼š"æˆ‘åªç›¸ä¿¡æµ·ã€‚"

è¿œå¤„ä¼ æ¥æ±½ç¬›å£°ï¼Œåˆ’ç ´äº†å¯‚é™çš„å¤œç©ºã€‚
"""
    
    script = director.parse_text_to_script(test_text)
    print("è§£æç»“æœ:")
    for i, unit in enumerate(script, 1):
        print(f"{i}. {unit}")