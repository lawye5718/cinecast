#!/usr/bin/env python3
"""
CineCast Qwen-Flash å¤§æ¨¡å‹å‰§æœ¬é¢„å¤„ç†å™¨ (ç®€åŒ–ç‰ˆ)
ä¸“ä¸ºå•†ä¸šAPIä¼˜åŒ–ï¼Œç§»é™¤æ‰€æœ‰å…è´¹æ¨¡å‹é™åˆ¶
"""

import json
import re
import logging
import os
import tempfile
from typing import List, Dict, Optional
from openai import OpenAI

logger = logging.getLogger(__name__)


def atomic_json_write(path: str, data, **kwargs) -> None:
    """Atomic JSON write: write to a temporary file first, then replace."""
    dir_name = os.path.dirname(path) or "."
    kwargs.setdefault("ensure_ascii", False)
    kwargs.setdefault("indent", 2)
    fd, tmp_path = tempfile.mkstemp(suffix=".tmp", dir=dir_name)
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, **kwargs)
        os.replace(tmp_path, path)
    except BaseException:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass
        raise


def repair_json_array(raw: str) -> Optional[List[Dict]]:
    """Attempt to repair a truncated or malformed JSON array."""
    raw = raw.strip()
    if raw.startswith("["):
        last_brace = raw.rfind("}")
        if last_brace > 0:
            candidate = raw[: last_brace + 1].rstrip().rstrip(",") + "\n]"
            try:
                result = json.loads(candidate)
                if isinstance(result, list) and result:
                    return result
            except json.JSONDecodeError:
                pass

    return salvage_json_entries(raw)


def _extract_fields_from_object(obj_text: str) -> Optional[Dict]:
    """Extract known fields from a single JSON object text in any order."""
    field_re = re.compile(r'"(\w+)"\s*:\s*"([^"]*)"')
    fields: Dict[str, str] = {}
    for m in field_re.finditer(obj_text):
        fields[m.group(1)] = m.group(2)

    speaker = fields.get("speaker", "")
    content = fields.get("content", "")
    if not speaker and not content:
        return None

    return {
        "type": fields.get("type", "narration") or "narration",
        "speaker": speaker or "narrator",
        "gender": fields.get("gender", "unknown") or "unknown",
        "emotion": fields.get("emotion") or fields.get("instruct") or "å¹³é™",
        "content": content or "",
    }


def salvage_json_entries(raw: str) -> Optional[List[Dict]]:
    """Use regex to extract valid script entries from broken JSON text."""
    obj_pattern = re.compile(r'\{[^{}]+\}', re.DOTALL)
    entries = []
    for m in obj_pattern.finditer(raw):
        entry = _extract_fields_from_object(m.group(0))
        if entry and entry.get("content"):
            entries.append(entry)

    if not entries:
        loose = re.compile(
            r'"speaker"\s*:\s*"([^"]*)"\s*[,}].*?"content"\s*:\s*"([^"]*)"',
            re.DOTALL,
        )
        for m in loose.finditer(raw):
            entries.append({
                "type": "narration",
                "speaker": m.group(1) or "narrator",
                "gender": "unknown",
                "emotion": "å¹³é™",
                "content": m.group(2) or "",
            })

    return entries if entries else None


class QwenScriptDirector:
    # é«˜é˜¶è§’è‰²éŸ³è‰²æ˜ å°„è¡¨
    VOICE_ARCHETYPES = {
        "intellectual": "Clear, articulate, mid-range voice, steady pacing, calm and intellectual.",
        "villain_sly": "Slightly nasal, fast-paced voice, bright tone, with a hint of sarcasm.",
        "melancholic": "Breathier, soft voice, melancholic undertones, slow and emotional.",
        "authoritative": "Resonant, deep baritone, slow and authoritative, gravelly texture.",
        "innocent": "Bright, high-pitched, energetic and innocent, clear enunciation.",
    }

    def __init__(self, api_key=None, global_cast=None, cast_db_path=None, **kwargs):
        self.api_key = api_key or os.environ.get("DASHSCOPE_API_KEY", "")
        if not self.api_key:
            logger.warning("âš ï¸ æœªè®¾ç½® DASHSCOPE_API_KEY")
            
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        
        self.model_name = "qwen-flash"
        self.max_chars_per_chunk = 60
        self.pure_narrator_chunk_limit = 100
        self.global_cast = global_cast or {}
        self.cast_db_path = cast_db_path or os.path.join("workspace", "cast_profiles.json")
        self.cast_profiles = self._load_cast_profiles()

    def _load_cast_profiles(self) -> Dict[str, Dict]:
        """åŠ è½½å·²ä¿å­˜çš„è§’è‰²éŸ³è‰²åº“"""
        if os.path.exists(self.cast_db_path):
            try:
                with open(self.cast_db_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, OSError) as e:
                logger.warning(f"âš ï¸ åŠ è½½è§’è‰²éŸ³è‰²åº“å¤±è´¥: {e}")
        return {}

    def reset_context(self) -> None:
        """é‡ç½®ä¸Šä¸‹æ–‡"""
        logger.info("â™»ï¸ é‡ç½®ä¸Šä¸‹æ–‡")

    def _chunk_text_for_llm(self, text: str, max_length: int = 997000) -> List[str]:
        """æŒ‰æ®µè½åˆ‡åˆ†ä¸ºå®‰å…¨å¤§å°ç»™ LLM å¤„ç†"""
        paragraphs = text.split('\n')
        chunks, current_chunk = [], ""
        for para in paragraphs:
            if not para.strip(): continue
            if len(current_chunk) + len(para) > max_length and current_chunk:
                chunks.append(current_chunk)
                current_chunk = para + "\n"
            else:
                current_chunk += para + "\n"
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    def verify_integrity(self, original_text: str, script_list: List[Dict]) -> bool:
        """å†…å®¹å®Œæ•´æ€§æ ¡éªŒ"""
        if not original_text or not script_list:
            return True
        content_text = "".join([item.get("content", "") for item in script_list])
        original_len = len(original_text.strip())
        if original_len == 0:
            return True
        ratio = len(content_text) / original_len
        if ratio < 0.9:
            logger.error(f"ğŸš¨ å†…å®¹ä¸¢å¤±ä¸¥é‡ï¼ä¿ç•™ç‡{ratio:.1%}")
            return False
        logger.info(f"âœ… å†…å®¹å®Œæ•´æ€§æ ¡éªŒé€šè¿‡ (ä¿ç•™ç‡{ratio:.1%})")
        return True
    
    def generate_pure_narrator_script(self, text: str, chapter_prefix: str = "chunk") -> List[Dict]:
        """çº¯å‡€æ—ç™½æ¨¡å¼ä¸“ç”¨çš„å‰§æœ¬ç”Ÿæˆå™¨"""
        micro_script = []
        chunk_id = 1
        pure_chunk_limit = self.pure_narrator_chunk_limit
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]

        for p_idx, para in enumerate(paragraphs):
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›.!?;])', para)
            temp_sentence = ""
            for part in sentences:
                if not part.strip() and not re.match(r'[ã€‚ï¼ï¼Ÿï¼›.!?;]', part):
                    continue

                if re.match(r'^[ã€‚ï¼ï¼Ÿï¼›.!?;]$', part.strip()):
                    temp_sentence += part
                    if len(temp_sentence) > pure_chunk_limit:
                        sub_parts = re.split(r'([ï¼Œã€ï¼š,:])', temp_sentence)
                        sub_temp = ""
                        for sub in sub_parts:
                            if re.match(r'^[ï¼Œã€ï¼š,:]$', sub):
                                sub_temp += sub
                                pause = self._calculate_pause(sub_temp, False)
                                micro_script.append({
                                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                                    "type": "narration",
                                    "speaker": "narrator",
                                    "gender": "male",
                                    "emotion": "å¹³é™",
                                    "content": sub_temp.strip(),
                                    "pause_ms": pause
                                })
                                chunk_id += 1
                                sub_temp = ""
                            else:
                                sub_temp += sub
                        if sub_temp.strip():
                            pause = self._calculate_pause(sub_temp, p_idx == len(paragraphs)-1)
                            micro_script.append({
                                "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                                "type": "narration",
                                "speaker": "narrator",
                                "gender": "male",
                                "emotion": "å¹³é™",
                                "content": sub_temp.strip(),
                                "pause_ms": pause
                            })
                            chunk_id += 1
                    else:
                        pause = self._calculate_pause(temp_sentence, p_idx == len(paragraphs)-1)
                        micro_script.append({
                            "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                            "type": "narration",
                            "speaker": "narrator",
                            "gender": "male",
                            "emotion": "å¹³é™",
                            "content": temp_sentence.strip(),
                            "pause_ms": pause
                        })
                        chunk_id += 1
                    temp_sentence = ""
                else:
                    temp_sentence += part

            if temp_sentence.strip():
                pause = self._calculate_pause(temp_sentence, p_idx == len(paragraphs)-1)
                micro_script.append({
                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                    "type": "narration",
                    "speaker": "narrator",
                    "gender": "male",
                    "emotion": "å¹³é™",
                    "content": temp_sentence.strip(),
                    "pause_ms": pause
                })
                chunk_id += 1

        return micro_script

    def parse_and_micro_chunk(self, text: str, chapter_prefix: str = "chunk", max_length: int = 997000) -> List[Dict]:
        """å®è§‚å‰§æœ¬è§£æ -> è‡ªåŠ¨å±•å¼€ä¸ºå¾®åˆ‡ç‰‡å‰§æœ¬"""
        macro_script = self.parse_text_to_script(text, max_length=max_length)
        micro_script = []
        chunk_id = 1
        smart_chunk_limit = max(self.max_chars_per_chunk, 90) 
        
        for unit in macro_script:
            content = unit.get("content", "")
            if not content or not content.strip():
                continue

            raw_sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›.!?;])', content)
            chunks = []
            temp = ""
            for part in raw_sentences:
                if not part.strip():
                    continue
                if re.match(r'^[ã€‚ï¼ï¼Ÿï¼›.!?;]$', part.strip()):
                    temp += part
                    if len(temp) <= smart_chunk_limit:
                        chunks.append(temp)
                        temp = ""
                    else:
                        sub_parts = re.split(r'([ï¼Œã€ï¼š,:])', temp)
                        sub_temp = ""
                        for sub in sub_parts:
                            if re.match(r'^[ï¼Œã€ï¼š,:]$', sub):
                                sub_temp += sub
                                chunks.append(sub_temp)
                                sub_temp = ""
                            else:
                                sub_temp += sub
                        if sub_temp:
                            chunks.append(sub_temp)
                        temp = ""
                else:
                    temp += part
            if temp: chunks.append(temp)
            
            valid_chunks = [c.strip() for c in chunks if c.strip()]

            if not valid_chunks and content.strip():
                hard_cut_chunk_size = smart_chunk_limit
                stripped = content.strip()
                valid_chunks = [
                    stripped[i:i + hard_cut_chunk_size]
                    for i in range(0, len(stripped), hard_cut_chunk_size)
                ]

            for idx, chunk in enumerate(valid_chunks):
                is_para_end = (idx == len(valid_chunks) - 1)
                pause_ms = self._calculate_pause(chunk, is_para_end)
                
                micro_script.append({
                    "chunk_id": f"{chapter_prefix}_{chunk_id:05d}",
                    "type": unit["type"],
                    "speaker": unit["speaker"],
                    "gender": unit.get("gender", "male"),
                    "content": chunk,
                    "pause_ms": pause_ms
                })
                chunk_id += 1
                
        return micro_script

    def _calculate_pause(self, chunk_text: str, is_para_end: bool) -> int:
        """è®¡ç®—åœé¡¿æ—¶é—´"""
        if is_para_end: return 1000
        if chunk_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')): return 600
        elif chunk_text.endswith(('ï¼›', ';')): return 400
        elif chunk_text.endswith(('ï¼Œ', 'ã€', ',', 'ï¼š', ':')): return 250
        else: return 100

    @staticmethod
    def _normalize_text(text: str) -> str:
        """æ•°å­—å’Œç¬¦å·è½¬æ¢ä¸ºä¸­æ–‡å¯è¯»å½¢å¼"""
        _DIGIT_MAP = {
            '0': 'é›¶', '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››',
            '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹',
        }

        def _digits_to_chinese(m: re.Match) -> str:
            s = m.group(0)
            return ''.join(_DIGIT_MAP.get(c, c) for c in s)

        text = re.sub(r'(\d+(?:\.\d+)?)%', lambda m: f'ç™¾åˆ†ä¹‹{"".join(_DIGIT_MAP.get(c, c) for c in m.group(1))}', text)
        text = re.sub(r'(\d+)\.(\d+)', lambda m: f'{"".join(_DIGIT_MAP.get(c, c) for c in m.group(1))}ç‚¹{"".join(_DIGIT_MAP.get(c, c) for c in m.group(2))}', text)
        text = re.sub(r'\d+', _digits_to_chinese, text)
        return text

    def parse_text_to_script(self, text: str, max_length: int = 997000) -> List[Dict]:
        """ä½¿ç”¨Qwen-Flashè¿›è¡Œå‰§æœ¬è§£æ"""
        logger.info(f"ğŸš€ å¯åŠ¨ Qwen-Flash å‰§æœ¬è§£æï¼Œå½“å‰ç« èŠ‚å­—æ•°: {len(text)}")
        text_chunks = self._chunk_text_for_llm(text, max_length=max_length)
        full_script = []
        
        for i, chunk in enumerate(text_chunks):
            logger.info(f"   ğŸ§  æ­£åœ¨è§£æå‰§æƒ…ç‰‡æ®µ {i+1}/{len(text_chunks)}...")
            chunk_script = self._request_llm(chunk)
            full_script.extend(chunk_script)
        
        if not full_script:
            raise RuntimeError("âŒ å‰§æœ¬è§£æç»“æœä¸ºç©º")
            
        if not self.verify_integrity(text, full_script):
            logger.warning("âš ï¸ å†…å®¹å®Œæ•´æ€§æ ¡éªŒæœªé€šè¿‡")
            
        return full_script
    
    def generate_chapter_recap(self, prev_chapter_text: str) -> str:
        """å‰æƒ…æ‘˜è¦ç”Ÿæˆ"""
        text = prev_chapter_text.strip()
        if not text:
            return ""

        logger.info(f"ğŸš€ å¯åŠ¨ Qwen-Flash å‰æƒ…æ‘˜è¦ç”Ÿæˆï¼Œä¸Šä¸€ç« å­—æ•°: {len(text)}")
        reduce_prompt = (
            'ä½ æ˜¯ä¸€ä½é¡¶çº§çš„æœ‰å£°ä¹¦å‰§æœ¬ç¼–è¾‘å’Œæ‚¬ç–‘å¤§å¸ˆã€‚'
            'è¯·æ ¹æ®æä¾›çš„ä¸Šä¸€ç« å†…å®¹ï¼Œå†™ä¸€æ®µä¸è¶…è¿‡100å­—çš„\u201cå‰æƒ…æ‘˜è¦\u201dã€‚'
            'ç»å¯¹çºªå¾‹ï¼š'
            '1. è¯­è¨€å¿…é¡»é«˜åº¦å‡ç»ƒï¼Œå…·æœ‰ç¾å‰§ç‰‡å¤´çš„ç”µå½±æ„Ÿï¼ˆ\u201cPreviously on...\u201dçš„é£æ ¼ï¼‰ã€‚'
            '2. åªä¿ç•™æœ€å…·å¼ åŠ›çš„å‰§æƒ…çŸ›ç›¾ã€‚'
            '3. æœ€åä¸€å¥å¿…é¡»æ˜¯ä¸€ä¸ªå¼•å‡ºä¸‹ä¸€ç« çš„\u201cæ‚¬å¿µé’©å­\u201dã€‚'
            '4. ç»å¯¹ä¸è¦è¾“å‡º\u201cå‰æƒ…æè¦ï¼š\u201dè¿™æ ·çš„æ ‡é¢˜ï¼Œç›´æ¥è¾“å‡ºæ­£æ–‡ã€‚'
        )

        try:
            completion = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": reduce_prompt},
                    {"role": "user", "content": f"ä¸Šä¸€ç« å†…å®¹ï¼š\n{text}"}
                ],
                extra_body={"enable_thinking": True},
                stream=False,
                temperature=0.5,
                top_p=0.8,
                max_tokens=32768,
            )
            recap_result = completion.choices[0].message.content.strip()
            recap_result = re.sub(r'^(å‰æƒ…æè¦|å‰æƒ…æ‘˜è¦|å›é¡¾|æ‘˜è¦)[:ï¼š]\s*', '', recap_result)
            return recap_result
        except Exception as e:
            logger.error(f"ç»ˆææ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _request_llm(self, text_chunk: str) -> List[Dict]:
        """å‘ Qwen-Flash API å‘é€è¯·æ±‚"""
        EMOTION_SET = "å¹³é™, æ„¤æ€’, æ‚²ä¼¤, å–œæ‚¦, ææƒ§, æƒŠè®¶, æ²§æ¡‘, æŸ”å’Œ, æ¿€åŠ¨, å˜²è®½, å“½å’½, å†°å†·, ç‹‚å–œ"

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„æœ‰å£°ä¹¦å‰§æœ¬è½¬æ¢æ¥å£ã€‚
ä»»åŠ¡ï¼šå°†è¾“å…¥æ–‡æœ¬é€å¥è§£æä¸º JSON æ•°ç»„æ ¼å¼ã€‚
æ ¸å¿ƒè§„åˆ™ï¼š
1. ç‰©ç†å¯¹é½ï¼šåŸæ–‡çš„æ¯ä¸€å¥ã€æ¯ä¸€æ®µå¿…é¡»å¯¹åº”æ•°ç»„ä¸­çš„ä¸€ä¸ªå¯¹è±¡ã€‚ä¸¥ç¦åˆå¹¶ï¼Œä¸¥ç¦åˆ å‡ã€‚
2. æ ¹èŠ‚ç‚¹çº¦æŸï¼šè¾“å‡ºç»“æœå¿…é¡»æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ JSON æ•°ç»„ï¼ˆå³ä»¥ `[` å¼€å¤´ï¼‰ã€‚ä¸¥ç¦è¾“å‡º `{{"data": [...]}}` è¿™ç§æ ¼å¼ã€‚
3. å­—æ®µè¦æ±‚ï¼šæ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å« type, speaker, gender, emotion, content å­—æ®µã€‚
4. è§’è‰²ä¸€è‡´æ€§ï¼šspeaker å¿…é¡»æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ã€‚
5. æƒ…ç»ªçº¦æŸï¼šä»…é™ [{EMOTION_SET}]ã€‚å¦‚ä¼´éšç‰¹å®šå‘éŸ³ç‰¹å¾ï¼ˆå¦‚"å¹æ°”", "ä½è¯­"ï¼‰ï¼Œå¯åœ¨æƒ…ç»ªååŠ æ‹¬å·è¯´æ˜ï¼Œä¾‹å¦‚ï¼š"æ‚²ä¼¤ (å¸¦å“­è…”)"ã€‚
"""

        one_shot_example = """
ã€è¾“å…¥ã€‘ï¼š
"ä½ å¥½ï¼Œ"è€æ¸”å¤«è¯´ã€‚ä»–çœ‹ç€å¤§æµ·ã€‚
ã€è¾“å‡ºã€‘ï¼š
[
  {"type": "dialogue", "speaker": "è€æ¸”å¤«", "gender": "male", "emotion": "å¹³é™", "content": "ä½ å¥½ï¼Œ"},
  {"type": "narration", "speaker": "narrator", "gender": "male", "emotion": "å¹³é™", "content": "è€æ¸”å¤«è¯´ã€‚"},
  {"type": "narration", "speaker": "narrator", "gender": "male", "emotion": "å¹³é™", "content": "ä»–çœ‹ç€å¤§æµ·ã€‚"}
]
"""

        if self.global_cast:
            cast_names = list(self.global_cast.keys())
            cast_info_parts = []
            for name, info in self.global_cast.items():
                if isinstance(info, dict):
                    g = info.get("gender", "unknown")
                    cast_info_parts.append(f'"{name}"(gender={g})')
                else:
                    cast_info_parts.append(f'"{name}"')
            cast_listing = ", ".join(cast_info_parts)
            system_prompt += f"""

ã€å…¨å±€é€‰è§’çºªå¾‹ï¼ˆCast Whitelistï¼‰ã€‘
- ä»¥ä¸‹æ˜¯æœ¬ä¹¦çš„å®˜æ–¹è§’è‰²åå•ï¼ˆæ ‡å‡†åï¼‰ï¼š{cast_listing}
- ä½ åœ¨ speaker å­—æ®µä¸­ä½¿ç”¨çš„è§’è‰²åï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°æ ‡å‡†åï¼
- ä¸¥ç¦è‡ªè¡Œå‘æ˜æˆ–ä½¿ç”¨ä»»ä½•ä¸åœ¨åå•ä¸­çš„è§’è‰²åï¼
- å¦‚æœé‡åˆ°åå•å¤–çš„é¾™å¥—è§’è‰²ï¼Œç»Ÿä¸€ä½¿ç”¨ "è·¯äºº" ä½œä¸º speakerã€‚
"""

        system_prompt += self._get_archetype_prompt()

        text_chunk = self._normalize_text(text_chunk)
        text_chunk = re.sub(r'"([^"]*)"', lambda m: '\u201c' + m.group(1) + '\u201d', text_chunk)
        text_chunk = text_chunk.replace('"', '\u2018')

        user_content = f"ã€æŒ‡ä»¤ï¼šå°†ä»¥ä¸‹æ–‡æœ¬è½¬æ¢ä¸ºå¹³é“ºçš„ JSON æ•°ç»„ï¼Œä¸¥ç¦æœ€å¤–å±‚ä½¿ç”¨å­—å…¸ã€‘\n\nå¾…å¤„ç†åŸæ–‡ï¼š\n{text_chunk}"

        try:
            completion = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt + "\nç¤ºä¾‹å‚è€ƒï¼š" + one_shot_example},
                    {"role": "user", "content": user_content}
                ],
                extra_body={"enable_thinking": True},
                stream=False,
                temperature=0.1,
                top_p=0.1,
                max_tokens=32768,
            )

            content = completion.choices[0].message.content.strip()
            content = content.replace('\t', ' ').replace('\r', '')
            content = re.sub(r'^```(?:json)?\s*', '', content.strip(), flags=re.IGNORECASE)
            content = re.sub(r'\s*```$', '', content.strip())

            try:
                script = json.loads(content)
            except json.JSONDecodeError:
                logger.warning("âš ï¸ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
                script = repair_json_array(content)
                if script is None:
                    logger.warning("âš ï¸ JSONå½»åº•æŸåï¼Œå¯ç”¨é™çº§æ–¹æ¡ˆ")
                    return self._validate_script_elements([
                        {"type": "narration", "speaker": "narrator", "content": text_chunk}
                    ])
                return self._validate_script_elements(script)

            if isinstance(script, list):
                return self._validate_script_elements(script)

            if isinstance(script, dict):
                if not script:
                    return self._validate_script_elements([
                        {"type": "narration", "speaker": "narrator", "content": text_chunk}
                    ])
                if "content" in script and "name" in script:
                    script = [{"type": "narration", "speaker": "narrator", "content": script["content"]}]
                    return self._validate_script_elements(script)
                if "content" in script or "type" in script:
                    return self._validate_script_elements([script])
                for value in script.values():
                    if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
                        return self._validate_script_elements(value)
                return self._validate_script_elements([
                    {"type": "narration", "speaker": "narrator", "content": text_chunk}
                ])
                    
            return self._validate_script_elements([
                {"type": "narration", "speaker": "narrator", "content": text_chunk}
            ])

        except Exception as e:
            raise RuntimeError(f"âŒ Qwen-Flash API è¯·æ±‚å¤±è´¥: {e}")

    def _get_archetype_prompt(self) -> str:
        """ç”ŸæˆéŸ³è‰²æ˜ å°„æŒ‡å—"""
        guidelines = "\n".join([f"  - {k}: {v}" for k, v in self.VOICE_ARCHETYPES.items()])
        return (
            "\nã€éŸ³è‰²è®¾è®¡å‚è€ƒæ‰‹å†Œã€‘\n"
            "å½“ä¸ºæ–°è§’è‰²ç”Ÿæˆ (Acoustic Description) æ—¶ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä»¥ä¸‹æ–‡å­¦åŸå‹æè¿°è¯ï¼š\n"
            f"{guidelines}\n"
        )
    
    def _validate_script_elements(self, script: List[Dict]) -> List[Dict]:
        """éªŒè¯å¹¶ä¿®å¤è„šæœ¬å…ƒç´ """
        required_fields = ['type', 'speaker', 'content']
        validated_script = []
        
        for i, element in enumerate(script):
            if not isinstance(element, dict):
                continue
                
            fixed_element = element.copy()

            if 'content' in fixed_element:
                if isinstance(fixed_element['content'], list):
                    fixed_element['content'] = "\n".join(str(x) for x in fixed_element['content'])
                elif not isinstance(fixed_element['content'], str):
                    fixed_element['content'] = str(fixed_element['content'])
            
            for field in required_fields:
                if field not in fixed_element:
                    if field == 'type':
                        fixed_element['type'] = 'narration'
                    elif field == 'speaker':
                        fixed_element['speaker'] = 'narrator'
                    elif field == 'content':
                        fixed_element['content'] = ''
            
            if fixed_element.get('speaker') is None:
                fixed_element['speaker'] = 'narrator'
            if 'gender' not in fixed_element:
                fixed_element['gender'] = 'unknown'
            if 'emotion' not in fixed_element:
                fixed_element['emotion'] = 'å¹³é™'

            validated_script.append(fixed_element)
            
        return validated_script
    

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    director = QwenScriptDirector()
    
    test_text = """
ç¬¬ä¸€ç«  å‡¯å¤«æ‹‰ç»´å…‹çš„é£é›ª

å¤œå¹•é™ä¸´ï¼Œæ¸¯å£çš„ç¯ç«å¼€å§‹é—ªçƒã€‚

"ä½ ç›¸ä¿¡å‘½è¿å—ï¼Ÿ"è€æ¸”å¤«è¯´é“ã€‚

å¹´è½»äººæ‘‡æ‘‡å¤´ï¼š"æˆ‘åªç›¸ä¿¡æµ·ã€‚"

è¿œå¤„ä¼ æ¥æ±½ç¬›å£°ï¼Œåˆ’ç ´äº†å¯‚é™çš„å¤œç©ºã€‚
"""
    
    script = director.parse_text_to_script(test_text)
    print("è§£æç»“æœ:")
    for i, unit in enumerate(script, 1):
        print(f"{i}. {unit}")