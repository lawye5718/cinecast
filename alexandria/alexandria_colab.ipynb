{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Alexandria Audiobook Generator — Google Colab\n\nRun [Alexandria](https://github.com/Finrandojin/alexandria-audiobook) on a free Google Colab GPU. No local installation required.\n\n> **Note:** This Colab notebook is provided for convenience so you can try Alexandria without local installation. It is not the primary focus of the project — for the best experience, install Alexandria locally via [Pinokio](https://pinokio.computer).\n\n**What this notebook does:**\n1. Checks your GPU runtime\n2. Installs Alexandria and all dependencies\n3. (Optional) Mounts Google Drive to cache TTS models across sessions\n4. Starts the Alexandria server and creates a public URL via ngrok\n\n**Before you start:**\n- Make sure you've selected a **GPU runtime**: Runtime → Change runtime type → T4 GPU\n- You need a free [ngrok account](https://dashboard.ngrok.com/signup) for the tunnel\n- You need an LLM server for script generation (see Cell 5 for options)\n\n**Colab T4 GPU (15 GB VRAM) recommended settings:**\n- Parallel Workers: 5-10\n- Max Chars/Batch: 1500-2000\n- Compile Codec: enabled (recommended for speed)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Runtime\n",
    "\n",
    "Verify that a GPU is available. If this cell fails, go to **Runtime → Change runtime type → T4 GPU**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Go to Runtime → Change runtime type → T4 GPU, then re-run this cell.\"\n",
    "    )\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "if vram_gb < 10:\n",
    "    print(\"WARNING: Less than 10 GB VRAM. Batch sizes will be very limited.\")\n",
    "    print(\"Use Parallel Workers: 2-3 and Max Chars/Batch: 1000 in Setup.\")\n",
    "elif vram_gb < 16:\n",
    "    print(\"T4 detected (15 GB). Recommended: Parallel Workers 5-10, Max Chars/Batch 1500-2000.\")\n",
    "else:\n",
    "    print(f\"{vram_gb:.0f} GB VRAM available. You can use higher batch sizes.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Alexandria\n",
    "\n",
    "Clones the repository and installs all Python dependencies. This takes 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "ALEXANDRIA_DIR = \"/content/Alexandria\"\n",
    "\n",
    "# Clone the repository\n",
    "if not os.path.exists(ALEXANDRIA_DIR):\n",
    "    !git clone https://github.com/Finrandojin/alexandria-audiobook.git {ALEXANDRIA_DIR}\n",
    "else:\n",
    "    print(f\"Alexandria already cloned at {ALEXANDRIA_DIR}\")\n",
    "    !cd {ALEXANDRIA_DIR} && git pull\n",
    "\n",
    "# Install dependencies (skip torch — Colab already has it)\n",
    "!pip install -q -r {ALEXANDRIA_DIR}/app/requirements.txt\n",
    "!pip install -q qwen-tts==0.1.1\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print()\n",
    "print(\"Installation complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Optional)\n",
    "\n",
    "TTS models are **~3.5 GB each** and download on first use. By default, they're lost when the Colab session ends.\n",
    "\n",
    "Mounting Google Drive caches models persistently so you don't re-download them every session.\n",
    "\n",
    "**Skip this cell** if you don't want to use Drive storage (models will re-download each session)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Set HuggingFace cache to Google Drive\n",
    "hf_cache = \"/content/drive/MyDrive/.cache/huggingface\"\n",
    "os.makedirs(hf_cache, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "print(f\"HuggingFace cache set to: {hf_cache}\")\n",
    "print(\"Models will persist across Colab sessions.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure ngrok Tunnel\n",
    "\n",
    "ngrok creates a public URL so you can access the Alexandria web UI from your browser.\n",
    "\n",
    "1. Sign up for a free account at [ngrok.com](https://dashboard.ngrok.com/signup)\n",
    "2. Copy your auth token from [dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
    "3. Paste it below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "NGROK_AUTH_TOKEN = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if not NGROK_AUTH_TOKEN:\n",
    "    raise ValueError(\n",
    "        \"Please set your ngrok auth token above.\\n\"\n",
    "        \"Get one free at: https://dashboard.ngrok.com/get-started/your-authtoken\"\n",
    "    )\n",
    "\n",
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "print(\"ngrok configured.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start Alexandria\n",
    "\n",
    "This cell:\n",
    "1. Writes a default config (local TTS mode, auto GPU device)\n",
    "2. Starts the Alexandria server in the background\n",
    "3. Opens an ngrok tunnel to the web UI\n",
    "\n",
    "**After running this cell**, click the ngrok URL to open the Alexandria web UI.\n",
    "\n",
    "### LLM Setup\n",
    "\n",
    "Alexandria needs an LLM server for script generation. In the web UI **Setup tab**, configure one of:\n",
    "\n",
    "| Provider | Base URL | API Key |\n",
    "|----------|----------|--------|\n",
    "| OpenAI | `https://api.openai.com/v1` | Your OpenAI API key |\n",
    "| DeepSeek | `https://api.deepseek.com/v1` | Your DeepSeek API key |\n",
    "| OpenRouter | `https://openrouter.ai/api/v1` | Your OpenRouter API key |\n",
    "| Ollama (see Cell 6) | `http://localhost:11434/v1` | `local` |\n",
    "\n",
    "Or any other OpenAI-compatible API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import json\nimport os\nimport subprocess\nimport time\nimport requests\nfrom pyngrok import ngrok\n\nALEXANDRIA_DIR = \"/content/Alexandria\"\nAPP_DIR = os.path.join(ALEXANDRIA_DIR, \"app\")\nCONFIG_PATH = os.path.join(APP_DIR, \"config.json\")\n\n# Write default config if none exists\nif not os.path.exists(CONFIG_PATH):\n    config = {\n        \"llm\": {\n            \"base_url\": \"http://localhost:11434/v1\",\n            \"api_key\": \"local\",\n            \"model_name\": \"\"\n        },\n        \"tts\": {\n            \"mode\": \"local\",\n            \"device\": \"auto\",\n            \"language\": \"English\",\n            \"parallel_workers\": 8,\n            \"compile_codec\": True,\n            \"sub_batch_enabled\": True,\n            \"sub_batch_min_size\": 4,\n            \"sub_batch_ratio\": 5,\n            \"sub_batch_max_chars\": 2000\n        }\n    }\n    with open(CONFIG_PATH, \"w\") as f:\n        json.dump(config, f, indent=2)\n    print(\"Default config written (local TTS, auto GPU).\")\nelse:\n    print(\"Existing config.json found, keeping it.\")\n\n# Start the server\nprint(\"Starting Alexandria server...\")\nserver_process = subprocess.Popen(\n    [\"python\", \"app.py\"],\n    cwd=APP_DIR,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1\n)\n\n# Wait for server to start\nfor i in range(30):\n    try:\n        r = requests.get(\"http://127.0.0.1:4200/api/config\", timeout=2)\n        if r.status_code == 200:\n            print(\"Server is running.\")\n            break\n    except:\n        pass\n    time.sleep(2)\nelse:\n    print(\"WARNING: Server may not have started. Check output below.\")\n\n# Open ngrok tunnel\npublic_url = ngrok.connect(4200)\nprint()\nprint(\"=\" * 60)\nprint(f\"Alexandria is running at: {public_url}\")\nprint(\"=\" * 60)\nprint()\nprint(\"Open the URL above in your browser.\")\nprint(\"Configure your LLM in the Setup tab before generating scripts.\")\nprint()\nprint(\"First TTS generation will download the model (~3.5 GB).\")\nprint(\"Check this cell's output for download progress.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. (Optional) Install Ollama for Local LLM\n\nIf you don't have a cloud LLM API, you can run Ollama on Colab for script generation.\n\n**Important — VRAM sharing:** Ollama and TTS both use the T4 GPU. A 7B model uses ~5 GB VRAM, leaving only ~10 GB for TTS. This **will cause out-of-memory crashes** during batch generation, especially with LoRA voices.\n\n**Workflow:** Use Ollama for script generation, then **run Cell 6b to stop Ollama** before starting TTS batch generation. The LLM is only needed during script generation.\n\n**Skip this cell** if you're using a cloud API (OpenAI, DeepSeek, etc.) — that's the easiest option on Colab."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import subprocess\nimport time\n\nOLLAMA_MODEL = \"qwen2.5:7b\"  # @param {type:\"string\"}\n\n# Install zstd (required by Ollama installer)\n!apt-get install -y -qq zstd\n\n# Install Ollama\nprint(\"Installing Ollama...\")\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama server in background\nprint(\"Starting Ollama server...\")\nollama_process = subprocess.Popen(\n    [\"ollama\", \"serve\"],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.DEVNULL\n)\ntime.sleep(3)\n\n# Pull the model\nprint(f\"Pulling {OLLAMA_MODEL} (this may take a few minutes)...\")\n!ollama pull {OLLAMA_MODEL}\n\nprint()\nprint(f\"Ollama is running with {OLLAMA_MODEL}.\")\nprint()\nprint(\"In Alexandria Setup tab, configure:\")\nprint(f\"  LLM Base URL: http://localhost:11434/v1\")\nprint(f\"  API Key: local\")\nprint(f\"  Model Name: {OLLAMA_MODEL}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6b. Stop Ollama (Free VRAM for TTS)\n\n**Run this cell after script generation is complete**, before starting batch TTS generation. This frees ~5 GB of VRAM that Ollama was using for the LLM.\n\nYou do NOT need to run this if you used a cloud LLM API instead of Ollama.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Server Logs\n",
    "\n",
    "Run this cell to see real-time server output (model loading, generation progress, errors).\n",
    "\n",
    "**Interrupt the cell** (stop button) to stop viewing logs. The server keeps running."
   ]
  },
  {
   "cell_type": "code",
   "source": "import subprocess\n\n# Stop Ollama server to free GPU memory for TTS\ntry:\n    subprocess.run([\"pkill\", \"-f\", \"ollama\"], timeout=5)\n    print(\"Ollama stopped. GPU memory freed for TTS.\")\nexcept:\n    print(\"Ollama was not running.\")\n\n# Verify VRAM is freed\nimport torch\nif torch.cuda.is_available():\n    free_mem = torch.cuda.mem_get_info()[0] / 1e9\n    total_mem = torch.cuda.mem_get_info()[1] / 1e9\n    print(f\"VRAM: {free_mem:.1f} GB free / {total_mem:.1f} GB total\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stream server output (interrupt to stop viewing, server keeps running)\n",
    "try:\n",
    "    for line in server_process.stdout:\n",
    "        print(line, end=\"\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped viewing logs. Server is still running.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stop Server\n",
    "\n",
    "Run this cell when you're done to clean up."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Kill ngrok tunnels\n",
    "ngrok.kill()\n",
    "print(\"ngrok tunnel closed.\")\n",
    "\n",
    "# Kill server\n",
    "try:\n",
    "    server_process.terminate()\n",
    "    server_process.wait(timeout=5)\n",
    "    print(\"Server stopped.\")\n",
    "except:\n",
    "    server_process.kill()\n",
    "    print(\"Server killed.\")\n",
    "\n",
    "# Kill Ollama if running\n",
    "try:\n",
    "    ollama_process.terminate()\n",
    "    print(\"Ollama stopped.\")\n",
    "except:\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}